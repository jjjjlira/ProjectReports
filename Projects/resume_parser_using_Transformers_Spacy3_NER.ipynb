{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaIDnYfuC37P"
   },
   "source": [
    "# AASD 4005 - Adv. Mathematical Concepts for Machine Learning\n",
    "\n",
    "Details:\n",
    "- Version: 1.0. \n",
    "- Date: 2022-11-20\n",
    "\n",
    "Group: \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpvO7SPj8DAs"
   },
   "source": [
    "## Resume Parsing and Entity Recognition (NER) using Transformers and Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSK-E51o8rjg"
   },
   "source": [
    "We will use **RoBERTa** model for NER.\n",
    "- **Named-entity recognition (NER)** (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. (Wikipedia: https://en.wikipedia.org/wiki/Named-entity_recognition)\n",
    "- **RoBERTa stands for Robustly Optimized BERT Pre-training Approach**. It was presented by researchers at Facebook and Washington University. The goal of this paper was to optimize the training of BERT architecture in order to take lesser time during pre-training. (GeekforGeeks: https://www.geeksforgeeks.org/overview-of-roberta-model/)\n",
    "- This code is an implementation based on the approach explained in you tube: \"Resume (CV) Parsing using Spacy 3 | NER Training in Spacy v3\". https://www.youtube.com/watch?v=WpaioLNsoGI\n",
    "- This implementation intends to feed the resume classification Final project of the course \"Advanced Mathematical Concepts for Machine Learning\" at George Brown College.\n",
    "- It was run in Google colab using Premium GPU runtime.\n",
    "## Architecture:\n",
    "Defined in **\"config.cfg\" file**:\n",
    "- **Tokenizer:** \"components.ner.model.tok2vec\".\n",
    "- **NER:** \"spacy.TransitionBasedParser.v2\". \n",
    "- **Transformer Model:** \"spacy-transformers.TransformerModel.v3\".\n",
    "- **Optimizer:** \"Adam.v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ks59g3zD-_Jk"
   },
   "source": [
    "### Installing Tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbiWJXhXHh9O",
    "outputId": "2749c04e-8a99-4935-ac79-67468a4f01b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting spacy_transformers\n",
      "  Downloading spacy_transformers-1.1.8-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy_transformers) (3.4.2)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from spacy_transformers) (1.12.1+cu113)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy_transformers) (2.4.5)\n",
      "Collecting transformers<4.22.0,>=3.4.0\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 82.4 MB/s \n",
      "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.8.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 74.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (1.0.3)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (4.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (2.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (0.4.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (3.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (2.0.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (21.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (8.1.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (1.10.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (1.21.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4.0.0,>=3.4.0->spacy_transformers) (3.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<4.0.0,>=3.4.0->spacy_transformers) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.4.0->spacy_transformers) (5.2.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (1.24.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (0.0.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 75.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy_transformers) (3.8.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy_transformers) (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy_transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.22.0,>=3.4.0->spacy_transformers) (2022.6.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 61.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.4.0->spacy_transformers) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<4.0.0,>=3.4.0->spacy_transformers) (2.0.1)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers, spacy-alignments, spacy-transformers\n",
      "Successfully installed huggingface-hub-0.11.0 spacy-alignments-0.8.6 spacy-transformers-1.1.8 tokenizers-0.12.1 transformers-4.21.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.4 MB 27.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.2\n",
      "    Uninstalling spacy-3.4.2:\n",
      "      Successfully uninstalled spacy-3.4.2\n",
      "Successfully installed spacy-3.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy_transformers\n",
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vybdJNHj-40Z"
   },
   "source": [
    "### Importing libraries and checking for mandatory GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ou081-7HILpa"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "isPOfw6iIza5",
    "outputId": "aa484b3d-6601-4544-8957-f644b50af020"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3.4.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Spacy version\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVO8a844I6OA",
    "outputId": "333b9e3e-c7eb-4bf6-c5b0-c0cc61776a07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 21 00:02:21 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0    49W / 400W |    658MiB / 40536MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check GPU to use (GPU is MANDATORY for training the model; otherwise it will thrown an error when starting the training)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mBlwOAC_bFM"
   },
   "source": [
    "### Setting up the environment to train the RoBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lztooROpRevY",
    "outputId": "cef5e821-fb1f-4994-870b-543c72bdeacb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mounting Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "92dTX6y-JENI",
    "outputId": "b01bcb46-b851-4108-f8a8-645d38a63ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CV-Parsing-using-Spacy-3'...\n",
      "remote: Enumerating objects: 82, done.\u001b[K\n",
      "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
      "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
      "remote: Total 82 (delta 16), reused 5 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (82/82), done.\n"
     ]
    }
   ],
   "source": [
    "# Cloning the resume data to train the model as well as the base configuration file \n",
    "!git clone https://github.com/laxmimerit/CV-Parsing-using-Spacy-3.git\n",
    "# move git clone repository to a directory inside the mounted drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QkWKtQ26Jq3Y"
   },
   "outputs": [],
   "source": [
    "#  Fetching the data to train the model\n",
    "# cv_data = json.load(open('/content/CV-Parsing-using-Spacy-3/data/training/train_data.json', 'r'))\n",
    "cv_data = json.load(open('/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/train_data.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mn-i4drUKZ2_",
    "outputId": "15fe9631-3909-447f-fd1a-e62f902fda15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  200 resumes manually annotated using \"Label Studio\" https://labelstud.io/\n",
    "len(cv_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZH4saiqfK0bP",
    "outputId": "37cb3a3b-0058-42d3-c542-4f19b3ae4639"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Govardhana K Senior Software Engineer  Bengaluru, Karnataka, Karnataka - Email me on Indeed: indeed.com/r/Govardhana-K/ b2de315d95905b68  Total IT experience 5 Years 6 Months Cloud Lending Solutions INC 4 Month • Salesforce Developer Oracle 5 Years 2 Month • Core Java Developer Languages Core Java, Go Lang Oracle PL-SQL programming, Sales Force Developer with APEX.  Designations & Promotions  Willing to relocate: Anywhere  WORK EXPERIENCE  Senior Software Engineer  Cloud Lending Solutions -  Bangalore, Karnataka -  January 2018 to Present  Present  Senior Consultant  Oracle -  Bangalore, Karnataka -  November 2016 to December 2017  Staff Consultant  Oracle -  Bangalore, Karnataka -  January 2014 to October 2016  Associate Consultant  Oracle -  Bangalore, Karnataka -  November 2012 to December 2013  EDUCATION  B.E in Computer Science Engineering  Adithya Institute of Technology -  Tamil Nadu  September 2008 to June 2012  https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Govardhana-K/b2de315d95905b68?isid=rex-download&ikw=download-top&co=IN   SKILLS  APEX. (Less than 1 year), Data Structures (3 years), FLEXCUBE (5 years), Oracle (5 years), Algorithms (3 years)  LINKS  https://www.linkedin.com/in/govardhana-k-61024944/  ADDITIONAL INFORMATION  Technical Proficiency:  Languages: Core Java, Go Lang, Data Structures & Algorithms, Oracle PL-SQL programming, Sales Force with APEX. Tools: RADTool, Jdeveloper, NetBeans, Eclipse, SQL developer, PL/SQL Developer, WinSCP, Putty Web Technologies: JavaScript, XML, HTML, Webservice  Operating Systems: Linux, Windows Version control system SVN & Git-Hub Databases: Oracle Middleware: Web logic, OC4J Product FLEXCUBE: Oracle FLEXCUBE Versions 10.x, 11.x and 12.x  https://www.linkedin.com/in/govardhana-k-61024944/',\n",
       " {'entities': [[1749, 1755, 'Companies worked at'],\n",
       "   [1696, 1702, 'Companies worked at'],\n",
       "   [1417, 1423, 'Companies worked at'],\n",
       "   [1356, 1793, 'Skills'],\n",
       "   [1209, 1215, 'Companies worked at'],\n",
       "   [1136, 1247, 'Skills'],\n",
       "   [928, 932, 'Graduation Year'],\n",
       "   [858, 889, 'College Name'],\n",
       "   [821, 856, 'Degree'],\n",
       "   [787, 791, 'Graduation Year'],\n",
       "   [744, 750, 'Companies worked at'],\n",
       "   [722, 742, 'Designation'],\n",
       "   [658, 664, 'Companies worked at'],\n",
       "   [640, 656, 'Designation'],\n",
       "   [574, 580, 'Companies worked at'],\n",
       "   [555, 572, 'Designation'],\n",
       "   [470, 493, 'Companies worked at'],\n",
       "   [444, 468, 'Designation'],\n",
       "   [308, 314, 'Companies worked at'],\n",
       "   [234, 240, 'Companies worked at'],\n",
       "   [175, 198, 'Companies worked at'],\n",
       "   [93, 136, 'Email Address'],\n",
       "   [39, 48, 'Location'],\n",
       "   [13, 37, 'Designation'],\n",
       "   [0, 12, 'Name']]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of data\n",
    "cv_data [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07knqB1vqbMQ"
   },
   "source": [
    "# Configuration for the model\n",
    "The recommended way to train your spaCy pipelines is via the spacy train command on the command line. It only needs a single config.cfg configuration file that includes all settings and hyperparameters. You can optionally overwrite settings on the command line, and load in a Python file to register custom functions and architectures. This quickstart widget helps you generate a starter config with the recommended settings for your specific use case. It’s also available in spaCy as the init config command. https://spacy.io/usage/training#basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6ISsZtNrJ6a"
   },
   "source": [
    "## Training configuration files \n",
    "Training config files include all settings and hyperparameters for training your pipeline. Instead of providing lots of arguments on the command line, you only need to pass your config.cfg file to spacy train. Under the hood, the training config uses the configuration system provided by our machine learning library Thinc. This also makes it easy to integrate custom models and architectures, written in your framework of choice. Some of the main advantages and features of spaCy’s training config are:\n",
    "\n",
    "- **Structured sections**. The config is grouped into sections, and nested sections are defined using the . notation. For example, [components.ner] defines the settings for the pipeline’s named entity recognizer. The config can be loaded as a Python dict.\n",
    "- **References** to registered functions. Sections can refer to registered functions like model architectures, optimizers or schedules and define arguments that are passed into them. You can also register your own functions to define custom architectures or methods, reference them in your config and tweak their parameters.\n",
    "- **Interpolation**. If you have hyperparameters or other settings used by multiple components, define them once and reference them as variables.\n",
    "Reproducibility with no hidden defaults. The config file is the “single source of truth” and includes all settings.\n",
    "- **Automated checks and validation**. When you load a config, spaCy checks if the settings are complete and if all values have the correct types. This lets you catch potential mistakes early. In your custom architectures, you can use Python type hints to tell the config which types of data to expect. https://spacy.io/usage/training#config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBsfaqIFK6RP",
    "outputId": "a02b33fa-933e-4a11-a9be-e357f213522a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "# Creating \"config.cfg\" file from \"base_config.cfg\"\n",
    "!python -m spacy init fill-config /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/base_config.cfg /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMZ7ooyEnXFw",
    "outputId": "629e4ed8-7276-42c2-a43d-47d238ede582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paths]\n",
      "train = null\n",
      "dev = null\n",
      "vectors = null\n",
      "init_tok2vec = null\n",
      "\n",
      "[system]\n",
      "gpu_allocator = \"pytorch\"\n",
      "seed = 0\n",
      "\n",
      "[nlp]\n",
      "lang = \"en\"\n",
      "pipeline = [\"transformer\",\"ner\"]\n",
      "batch_size = 128\n",
      "disabled = []\n",
      "before_creation = null\n",
      "after_creation = null\n",
      "after_pipeline_creation = null\n",
      "tokenizer = {\"@tokenizers\":\"spacy.Tokenizer.v1\"}\n",
      "\n",
      "[components]\n",
      "\n",
      "[components.ner]\n",
      "factory = \"ner\"\n",
      "incorrect_spans_key = null\n",
      "moves = null\n",
      "scorer = {\"@scorers\":\"spacy.ner_scorer.v1\"}\n",
      "update_with_oracle_cut_size = 100\n",
      "\n",
      "[components.ner.model]\n",
      "@architectures = \"spacy.TransitionBasedParser.v2\"\n",
      "state_type = \"ner\"\n",
      "extra_state_tokens = false\n",
      "hidden_width = 64\n",
      "maxout_pieces = 2\n",
      "use_upper = false\n",
      "nO = null\n",
      "\n",
      "[components.ner.model.tok2vec]\n",
      "@architectures = \"spacy-transformers.TransformerListener.v1\"\n",
      "grad_factor = 1.0\n",
      "pooling = {\"@layers\":\"reduce_mean.v1\"}\n",
      "upstream = \"*\"\n",
      "\n",
      "[components.transformer]\n",
      "factory = \"transformer\"\n",
      "max_batch_items = 4096\n",
      "set_extra_annotations = {\"@annotation_setters\":\"spacy-transformers.null_annotation_setter.v1\"}\n",
      "\n",
      "[components.transformer.model]\n",
      "@architectures = \"spacy-transformers.TransformerModel.v3\"\n",
      "name = \"roberta-base\"\n",
      "mixed_precision = false\n",
      "\n",
      "[components.transformer.model.get_spans]\n",
      "@span_getters = \"spacy-transformers.strided_spans.v1\"\n",
      "window = 128\n",
      "stride = 96\n",
      "\n",
      "[components.transformer.model.grad_scaler_config]\n",
      "\n",
      "[components.transformer.model.tokenizer_config]\n",
      "use_fast = true\n",
      "\n",
      "[components.transformer.model.transformer_config]\n",
      "\n",
      "[corpora]\n",
      "\n",
      "[corpora.dev]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.dev}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[corpora.train]\n",
      "@readers = \"spacy.Corpus.v1\"\n",
      "path = ${paths.train}\n",
      "max_length = 0\n",
      "gold_preproc = false\n",
      "limit = 0\n",
      "augmenter = null\n",
      "\n",
      "[training]\n",
      "accumulate_gradient = 3\n",
      "dev_corpus = \"corpora.dev\"\n",
      "train_corpus = \"corpora.train\"\n",
      "seed = ${system.seed}\n",
      "gpu_allocator = ${system.gpu_allocator}\n",
      "dropout = 0.1\n",
      "patience = 1600\n",
      "max_epochs = 0\n",
      "max_steps = 20000\n",
      "eval_frequency = 200\n",
      "frozen_components = []\n",
      "annotating_components = []\n",
      "before_to_disk = null\n",
      "\n",
      "[training.batcher]\n",
      "@batchers = \"spacy.batch_by_padded.v1\"\n",
      "discard_oversize = true\n",
      "size = 2000\n",
      "buffer = 256\n",
      "get_length = null\n",
      "\n",
      "[training.logger]\n",
      "@loggers = \"spacy.ConsoleLogger.v1\"\n",
      "progress_bar = false\n",
      "\n",
      "[training.optimizer]\n",
      "@optimizers = \"Adam.v1\"\n",
      "beta1 = 0.9\n",
      "beta2 = 0.999\n",
      "L2_is_weight_decay = true\n",
      "L2 = 0.01\n",
      "grad_clip = 1.0\n",
      "use_averages = false\n",
      "eps = 0.00000001\n",
      "\n",
      "[training.optimizer.learn_rate]\n",
      "@schedules = \"warmup_linear.v1\"\n",
      "warmup_steps = 250\n",
      "total_steps = 20000\n",
      "initial_rate = 0.00005\n",
      "\n",
      "[training.score_weights]\n",
      "ents_f = 1.0\n",
      "ents_p = 0.0\n",
      "ents_r = 0.0\n",
      "ents_per_type = null\n",
      "\n",
      "[pretraining]\n",
      "\n",
      "[initialize]\n",
      "vectors = ${paths.vectors}\n",
      "init_tok2vec = ${paths.init_tok2vec}\n",
      "vocab_data = null\n",
      "lookups = null\n",
      "before_init = null\n",
      "after_init = null\n",
      "\n",
      "[initialize.components]\n",
      "\n",
      "[initialize.tokenizer]"
     ]
    }
   ],
   "source": [
    "!cat /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/config.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inqSR17gB84X"
   },
   "source": [
    "## Preparing Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGl2CYR5CEvn"
   },
   "source": [
    "Training data for NLP projects comes in many different formats. For some common formats such as CoNLL, spaCy provides converters you can use from the command line. In other cases you’ll have to prepare the training data yourself.\n",
    "\n",
    "When converting training data for use in spaCy, the main thing is to create Doc objects just like the results you want as output from the pipeline. For example, if you’re creating an NER pipeline, loading your annotations and setting them as the .ents property on a Doc is all you need to worry about. On disk the annotations will be saved as a DocBin in the .spacy format, but the details of that are handled automatically.\n",
    "\n",
    "(https://spacy.io/usage/training#training-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3nXEzJbwfQ9D"
   },
   "outputs": [],
   "source": [
    "def get_spacy_doc (file, data):\n",
    "  nlp = spacy.blank('en')\n",
    "  db = DocBin() # docbin object\n",
    "\n",
    "  for text, annot in tqdm(data):\n",
    "    doc =nlp.make_doc(text)\n",
    "    annot = annot ['entities']\n",
    "\n",
    "    ents = []\n",
    "    entity_indices = []\n",
    "\n",
    "    # skip overlapping annotated entitites\n",
    "    for start, end, label in annot:\n",
    "      skip_entity = False\n",
    "\n",
    "      for idx in range(start, end):\n",
    "        if idx in entity_indices:\n",
    "          skip_entity = True\n",
    "          break\n",
    "        if skip_entity == True:\n",
    "          continue\n",
    "\n",
    "        entity_indices  = entity_indices + list(range(start, end))\n",
    "\n",
    "        try:\n",
    "          span = doc.char_span(start, end, label=label, alignment_mode='strict')    \n",
    "        except:\n",
    "          continue\n",
    "        \n",
    "        if span == None:\n",
    "          err_data = str([start, end]) +  \"   \" + str(text) + '\\n'\n",
    "          file.write(err_data)\n",
    "        else:\n",
    "          ents.append(span)\n",
    "      \n",
    "      try:\n",
    "        doc.ents = ents\n",
    "        db.add(doc)\n",
    "      except:\n",
    "        pass\n",
    "\n",
    "  return db   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CKpZOQQFXsSE"
   },
   "outputs": [],
   "source": [
    "# Generating training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split (cv_data, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gp8Brf4ciNL_",
    "outputId": "54d1fe47-e6b5-48c4-d7e2-f8ca72d574f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 60)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7FtUxVonii-H",
    "outputId": "3001390e-7981-419e-d70f-6b0360444da5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [00:05<00:00, 24.68it/s]\n",
      "100%|██████████| 60/60 [00:01<00:00, 32.16it/s]\n"
     ]
    }
   ],
   "source": [
    "file =open('error.txt', 'w')  # Potencial errors in CVs entity annotation are saved in 'error.txt'\n",
    "\n",
    "db = get_spacy_doc(file, train)\n",
    "db.to_disk('train_data.spacy')\n",
    "\n",
    "db = get_spacy_doc(file, test)\n",
    "db.to_disk('test_data.spacy')\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tae8ZuX5ijAW",
    "outputId": "9d1d8dbf-b9ed-429c-cc4f-a619ce90eb9e"
   },
   "outputs": [],
   "source": [
    "# db.tokens  # data stored into db.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2r2h-tlmLOc",
    "outputId": "8f6754ca-e3fa-4fa0-e72f-a9d1c08632c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "728"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k2aSBcNmMCj",
    "outputId": "1db1abc0-193c-4723-8912-60f47b92e529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory:\n",
      "/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2022-11-21 00:03:32,200] [INFO] Set up nlp object from config\n",
      "INFO:spacy:Set up nlp object from config\n",
      "[2022-11-21 00:03:32,212] [INFO] Pipeline: ['transformer', 'ner']\n",
      "INFO:spacy:Pipeline: ['transformer', 'ner']\n",
      "[2022-11-21 00:03:32,217] [INFO] Created vocabulary\n",
      "INFO:spacy:Created vocabulary\n",
      "[2022-11-21 00:03:32,218] [INFO] Finished initializing nlp object\n",
      "INFO:spacy:Finished initializing nlp object\n",
      "Downloading config.json: 100% 481/481 [00:00<00:00, 471kB/s]\n",
      "Downloading vocab.json: 100% 878k/878k [00:01<00:00, 666kB/s]\n",
      "Downloading merges.txt: 100% 446k/446k [00:01<00:00, 413kB/s]\n",
      "Downloading tokenizer.json: 100% 1.29M/1.29M [00:01<00:00, 1.22MB/s]\n",
      "Downloading pytorch_model.bin: 100% 478M/478M [00:06<00:00, 73.2MB/s]\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[2022-11-21 00:04:48,841] [INFO] Initialized pipeline components: ['transformer', 'ner']\n",
      "INFO:spacy:Initialized pipeline components: ['transformer', 'ner']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['transformer', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.0\u001b[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0        2192.58   1599.49    0.11    0.06    5.01    0.00\n",
      "  0     200      115895.59  69854.38    4.71    3.44    7.47    0.05\n",
      "  0     400       23323.74  29647.19    0.69    0.46    1.41    0.01\n",
      "  0     600        5839.30  29948.43    9.47    5.99   22.55    0.09\n",
      "  1     800        7032.76  26939.45    8.59    5.54   19.19    0.09\n",
      "  1    1000       23060.05  28199.00   12.29    7.78   29.21    0.12\n",
      "  1    1200       22288.83  27053.12   28.01   33.86   23.88    0.28\n",
      "  1    1400        4592.88  25723.35   28.29   28.88   27.72    0.28\n",
      "  2    1600       12105.82  23983.09   25.15   33.91   19.99    0.25\n",
      "  2    1800       10587.71  24435.12   15.19   31.66   10.00    0.15\n",
      "  2    2000        1875.16  22630.61   11.76   30.99    7.26    0.12\n",
      "  2    2200       15721.94  23413.61   22.56   39.05   15.86    0.23\n",
      "  3    2400        2131.74  20500.88   17.07   31.54   11.70    0.17\n",
      "  3    2600        3016.76  20545.19   14.40   34.52    9.10    0.14\n",
      "  3    2800       21505.78  19859.92   17.92   33.10   12.28    0.18\n",
      "  3    3000        4021.89  18310.96   16.85   40.22   10.66    0.17\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/output/model-last\n",
      "CPU times: user 11.4 s, sys: 2.26 s, total: 13.7 s\n",
      "Wall time: 37min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# !python -m spacy train /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/config.cfg --output ./output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0\n",
    "!python -m spacy train /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/training/config.cfg --output /content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/output --paths.train ./train_data.spacy --paths.dev ./test_data.spacy --gpu-id 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZl9hbdAqkKS"
   },
   "source": [
    "## Quick Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "FR6sfs94qn2X"
   },
   "outputs": [],
   "source": [
    "# Loading the best trained model\n",
    "nlp = spacy.load('/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/output/model-best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jVdez_sH2Lbj"
   },
   "outputs": [],
   "source": [
    "# Short example\n",
    "doc = nlp('My name is John Smith. I worked at IBM. I have 15 years of experience')\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, \"   ->>>>   \", ent.label_)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9G6BKpdsIPW"
   },
   "source": [
    "## Preparing to read a real PDF resume\n",
    "## using PyMuPDF:\n",
    "With PyMuPDF you can access files with extensions like “.pdf”, “.xps”, “.oxps”, “.cbz”, “.fb2”, “.mobi” or “.epub”. In addition, about 10 popular image formats can also be opened and handled like documents. https://pymupdf.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6_ex8QC2Liy",
    "outputId": "f1af8f5e-a2db-4e2a-b291-8123c4adc9cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.21.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.0 MB 20.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.21.0\n"
     ]
    }
   ],
   "source": [
    "# Installing the library\n",
    "!pip install PyMuPDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NwKobEUS6J7U"
   },
   "outputs": [],
   "source": [
    "import sys, fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rwF4YeWo7CRC"
   },
   "outputs": [],
   "source": [
    "# Checking with a real uploaded Resume\n",
    "fname = '/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/ResumeAdvMath.pdf'\n",
    "doc =fitz.open(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Dcwot0z4sVXm"
   },
   "outputs": [],
   "source": [
    "# doc =[page.getText() for page in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddcIDuj-MmF_",
    "outputId": "f15a2d33-1511-4a1b-cd7e-1ad2b123af29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document('/content/drive/MyDrive/KaggleCompetitions/CV-Parsing-using-Spacy-3/data/ResumeAdvMath.pdf')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "pqehvNGzsu-S"
   },
   "outputs": [],
   "source": [
    "# Generating the text file \n",
    "text =''\n",
    "for page in doc:\n",
    "  text =text + str(page.get_text())\n",
    "text = text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KV6r7Y7QsVe2"
   },
   "outputs": [],
   "source": [
    "# text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5YNDsPXHtp-i",
    "outputId": "7b384338-45bc-4fa3-93d0-9cc131b1763a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Juan Perez \\nARTIFICIAL INTELLIGENCE \\nMBA - CYBERSECURITY \\nELECTRONICS ENGINEER \\n              \\n \\nMobile: +1 182-647-9981 \\nE-mail: juanperez1981@georgebrown.ca\\n \\nSUMMARY OF SKILLS:  \\nMachine Learning, Neural Networks, Deep Learning, NLP, TensorFlow, Python, C++, SQL, Tableau, AWS, Azure, Cloud, \\nCybersecurity, Kali Linux, Leadership, Technical Sales, English (Advanced), French (Intermediate), Spanish (Native).  \\n \\nCERTIFICATIONS:  \\n• \\nAWS Certified Machine Learning Specialty (2022)  \\n• \\nAWS Certified Cloud Solutions Architect Associate (2020)  \\n• \\nAzure Fundamentals (2022) \\n• \\nHCIA Huawei Certified ICT Associate - Cloud Services (2021) \\n \\nEDUCATION: \\nGeorge Brown College  \\n \\n \\n \\n \\n \\n \\n \\nToronto, Canada \\nApplied A.I. Solutions Development Program (Postgraduate) \\n \\n  \\n \\nAugust 2023 \\n \\nCentennial College  \\n \\n \\n \\n \\n \\n \\n \\nToronto, Canada \\nGraduate Certificate in Cybersecurity (with Honours)  \\n  \\n \\n \\nMay 2022 \\n \\nTexas A&M University, Mays School of Business \\n \\n \\n \\n \\nCollege Station, USA \\nMaster in Business Administration (MBA) \\n \\n  \\n \\n \\n \\nMay 2010 \\n \\nPontificia Universidad Católica del Peru  \\n \\n \\n \\n \\n \\nLima, Peru \\nB.S. Electronics Engineering \\n \\n \\n  \\n \\n \\n \\nMay 2005 \\n \\nAPPLIED RESEARCH AND PROJECTS: \\n• \\n“Intrusion Detection System using Machine Learning”. Capstone Project. Graduate Certificate in Cybersecurity. \\nCentennial College (Toronto, ON, Canada). \\n• \\n“Classification of Resumes using NLP”. Final Project Advanced Mathematics for Machine Learning. Applied A.I. \\nSolutions Development Program. George Brown College (Toronto, ON, Canada). \\n• \\n“Benchmarking NLP Feature Extractors for Classification”. Project Machine Learning 2. Applied A.I. Solutions \\nDevelopment Program. George Brown College (Toronto, ON, Canada). \\n• \\n“Penetration Testing using Python”. Final Project: Ethical Hacking and Defense. Graduate Certificate in \\nCybersecurity. Centennial College (Toronto, ON, Canada). \\n• \\n“IBM Communications Server Redbook for OS2/Warp version 4.1 Enhancements”. IBM International Technical \\nSupport Organization (Raleigh, NC. USA). \\n• \\n“Voice Compression System based on a time-variable Model of the Vocal Tract using Linear Prediction”. \\nUndergraduate Thesis for Electronics Engineer: Universidad Catolica del Peru (Lima, Peru). \\n \\nWORKING EXPERIENCE: \\n \\nHuawei Technologies \\n \\n \\n \\n \\n \\n \\n \\nShen Zhen, China \\nSales Director B2B and Cloud Services (Enterprise Business Group)  \\n \\n2018 – 2021 \\n• \\nWon migrations of Cloud Computing Services from AWS to Huawei Cloud in the Education Sector (UC, UTP, UNAP, \\netc.) with 7-digit Full Contract Value. \\n• \\nWon critical projects with Telefonica like “National Judicial Branch” (IP Telephony + Networking), Southern Copper \\nCorporation (Private LTE network), Telefonica (Routers CPEs) among others with 7-digit Full Contract Value each. \\nVice-Director Telefonica Key Account (Carrier Network Business Group) \\n \\n2013 – 2018 \\n• \\nWon strategic projects like Radio Access Network (RAN), DWDM, Energy Cabinets, First Line Maintenance (FLM), \\nData Center Tier III expansions, microwave links in Amazon rainforest, among others. \\n• \\nDeveloped key Customer Relationships with strategic top executives to win critical projects. \\nSales Director B2B (Enterprise Business Unit) \\n \\n \\n \\n \\n2011 – 2012   \\n• \\nCreated EBG area in Peru (Defined strategy and managed operations). \\n \\n \\n• \\nDeveloped market-winning key projects like Telefonica (Routers Run Rate), Interbank (IP Networking), Central Bank - \\nBCRP (DWDM), Aspersud (IP Networking), among others.  \\nKey Account Manager (Consumer Business Unit) \\n \\n \\n \\n \\n2009 – 2011  \\n• \\nIncreased sales of smartphones and tablets from 10,000 units to 600,000 units per year. \\n• \\nRebuilt Customer Relationships with key executives in Telefonica to develop and lead the B2C market. \\n \\nTelefonica \\n \\n \\n \\n \\n \\n \\n \\n \\nLima, Peru \\nMarketing Manager – Product Development \\n  \\n \\n \\n \\n2001 – 2008 \\n• \\nIncreased digital services revenue from US$2.11 million in 2002 to US$ 62 million in 2007. \\n• \\nLaunched strategic value-added services (designed commercial features, assessed CAPEX and OPEX budgets, and \\ndeveloped promotions): Videoplay, Mobile TV, Fulltracks downloads, Real Tones Downloads, SIM Cards Services, \\nRing Back Tones, Instant Messaging, Interactive TV contests, among others. \\n• \\nLeaded Data Services Projects and web interactive services like “SMS Premium Content”, “SMS alerts and \\nSubscription”, “SMS/WAP/Web Portals”. \\nTenfold Corp. \\n \\n \\n \\n \\n \\n \\n \\n \\nTexas. USA \\nApplication Developer \\n \\n \\n \\n \\n \\n \\n \\n2000 \\n• \\nAnalyzed, designed, developed, tested and implemented mission-critical software applications using patented \\n“Universal Application”.  Worked on projects for insurance companies “Utica” and “Royal”. \\n• \\nResearched customers’ business processes to define and document requirements for software applications \\n \\nTexas A&M University - MIS Program  \\n \\n \\n \\n \\nTexas. USA \\nGraduate Teaching Assistant: “Data Communications and Networking” \\n \\n1998 – 2000 \\n• \\nManaged a Networking Lab deploying and configuring Windows and Linux Servers, DHCP, DNS, and Email Server. \\n• \\nLectured twice a month about Networking configuration and practical experiences to MIS graduate students. \\n \\nIBM \\n \\n  \\n \\n \\n \\n \\n \\n \\n \\nLima, Peru \\nTechnical Support and Pre-Sales Representative  \\n \\n \\n \\n \\n1995 – 1997 \\n• \\nInstalled and configured PC servers, Routers and Switches, and “IBM Communication Server” gateways to allow \\nSNA and IP connectivity. \\n• \\nMaintained and supported PC Servers for corporate customers during normal hours and maintenance windows. \\n• \\nTrained internal employees and customers in OS/2 Warp Operating System and Networking. \\n \\nSofttek  \\n  \\n \\n \\n \\n \\n \\n \\n \\nLima, Peru \\nSoftware Engineer \\n \\n \\n  \\n \\n \\n \\n \\n1995 \\n• \\nDeveloped software in C++ for NCR Points of Sale (POS). \\n• \\nDeveloped software in FoxPro for Cash Flow for “Grupo Gloria Corp”. \\n \\nSPECIALIZATIONS AND COURSES: \\n• \\nNatural Language Processing: NLP 101, NLP 102, NLP 103, NLP 104. PyImageSearch (2022) \\n• \\nTensorFlow for Data and Deployment Specialization. DeepLearning.AI (2022) \\n• \\nTensorFlow Developer Professional Certificate. DeepLearning.AI (2022) \\n• \\nAWS Machine Learning Foundations Nanodegree. Udacity (2022) \\n• \\nIntroduction à l'Apprentissage Profonde. Université de Montréal (2022) \\n• \\nPython for Cybersecurity Specialization INFOSEC (2022) \\n• \\nBuilding Modern Python Applications on AWS (2021) \\n• \\nGoogle IT Automation with Python - Professional Certificate (2021) \\n• \\nAccelerated Computer Science Fundamentals - Specialization (2020) \\n• \\nDiscrete Mathematics for Computer Science Specialization (2021) \\n• \\nGoogle IT Support - Professional Certificate (2020) \\n• \\nIBM Design Thinking Practitioner (2022) \\n• \\nDesign Thinking for Innovation. University of Virginia (2021) \\n• \\nDigital transformation. University of Virginia (2019)\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I447AJBJtt0F",
    "outputId": "447a6e16-d13c-4e4c-9a8f-d6b348a7537f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning, Neural Networks, Deep Learning, NLP, TensorFlow, Python, C++, SQL, Tableau, AWS, Azure, Cloud, \n",
      "Cybersecurity, Kali Linux, Leadership, Technical Sales, English (Advanced), French (Intermediate), Spanish (Native)     ->>>>    Skills\n",
      "George Brown College     ->>>>    College Name\n",
      "Toronto     ->>>>    Location\n",
      "Centennial College     ->>>>    College Name\n",
      "Toronto     ->>>>    Location\n",
      "Texas A&M University, Mays School of Business     ->>>>    College Name\n",
      "Centennial College     ->>>>    College Name\n",
      "George Brown College     ->>>>    College Name\n",
      "George Brown College     ->>>>    College Name\n",
      "Centennial College     ->>>>    College Name\n",
      "Texas A&M University     ->>>>    College Name\n",
      "Lima     ->>>>    Location\n"
     ]
    }
   ],
   "source": [
    "# Entity Recognition of PDF Resume\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, \"    ->>>>   \", ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6XWp_DXr7Cmz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
