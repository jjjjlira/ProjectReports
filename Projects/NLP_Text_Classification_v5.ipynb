{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60058d0",
   "metadata": {},
   "source": [
    "# Task 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67af1d4",
   "metadata": {},
   "source": [
    "\n",
    "Create a benchmark analysis with different algorithms and feature extractors:\n",
    "\n",
    "Dataset: Fetch 20 Newsgroups (same as in class work)\n",
    "Algorithms: Multinomial Na√Øve Bayes, Logistic Regression, Support Vector Machines, Decision Trees\n",
    "Feature Extractors: CountVectorizer, Word2Vec, Doc2Vec and so on\n",
    "\n",
    "Benchmark all the possible above configurations and choose the best algorithm and feature extractor amongst all configurations and put it in a .txt or .doc file in a tabular format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c88d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "from time import time\n",
    "# import logging\n",
    "import warnings\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import utils\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.tokenize import word_tokenize\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "# To see all the content in the column\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a1dabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some categories from the training set\n",
    "categories = [\n",
    "              'comp.graphics',\n",
    "              'talk.politics.guns'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f405bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['comp.graphics', 'talk.politics.guns']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407514f8",
   "metadata": {},
   "source": [
    "### Fetch documents for these 2 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46b444d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(f\"{len(data.filenames)} documents\")\n",
    "print(f\"{len(data.target_names)} categories\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89afbb2",
   "metadata": {},
   "source": [
    "### Checking content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49e56807-bb86-43df-aa19-686ecaa65e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "625ada10-06dc-48d7-b408-45a444862564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: jmd@cube.handheld.com (Jim De Arras)\\nSubject: Re: FYI - BATF reply on Waco\\nOrganization: Hand Held Products, Inc.\\nLines: 52\\nDistribution: world\\nNNTP-Posting-Host: dale.handheld.com\\n\\nIn article <C5L480.K7u@elite.intel.com> dgw@elite.intel.com (Dennis Willson)  \\nwrites:\\n[..]\\n> \\n> On February 28, 1993, the special agents attempting to serve the\\n> Federal search warrant were all dressed in apparel clearly identified\\n> with the letters \"ATF\" and a highly visible police-type badge.\\n> Additionally, the special agents announced who they were and their\\n> purpose for being at the compound.\\n> \\n> Immediately following this announcement, gunfire erupted from the\\n> compound, resulting in the deaths of four ATF special agents and the\\n> wounding of several others.  Through no fault of ATF, the element of\\n> surprise was lost, which caused the tragedy.  \\n\\nThis statement simply amazes me!  \"Through no fault of ATF, the element of  \\nsurprise was lost\"!  What element of surprise?  In the paragraph preceding this  \\none, he said \"... the special agents announced who they were and their purpose  \\nfor being at the compound\", which was to serve the federal warrant.  No element  \\nof surprise was even needed for that.  \\n\\nNo, the element of surprise that they lost was that needed for a preemptive  \\nfirst strike, without warning.  \\n\\n> Inasmuch as the warrants\\n> remain sealed by a U.S. magistrate, and the investigation remains in an\\n> active ongoung status, we are prohibited from disclosing any further\\n> information at this time.\\n> \\n\\nRead: They need to wait until they see how it comes out before they fabricate  \\nanymore, which could get disproven.\\n\\n> We hope we have been responsive to your letter.  Please let us know\\n> whenever we may be of service.\\n> \\n>                           Sincerely yours,\\n> \\n>                           Daniel M. H??l??tt  [can\\'t make out signature]\\n>                           Deputy Director\\n\\nAs always, no facts, just my opinions/observations.\\n\\nJim\\n--\\njmd@handheld.com\\n-------------------------------------------------------------------------------\\n\"I\\'m always rethinking that.  There\\'s never been a day when I haven\\'t rethought  \\nthat.  But I can\\'t do that by myself.\"  Bill Clinton  6 April 93\\n\"If I were an American, as I am an Englishman, while a foreign troop was landed  \\nin my country, I never would lay down my arms,-never--never--never!\"\\nWILLIAM PITT, EARL OF CHATHAM 1708-1778 18 Nov. 1777\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1560f7cd",
   "metadata": {},
   "source": [
    "### Define a pipeline combining a text feature extractor with a simple classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58959f",
   "metadata": {},
   "source": [
    "### Find the best parameters for both the feature extraction and the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce985e0",
   "metadata": {},
   "source": [
    "# Defining Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64cd5d2",
   "metadata": {},
   "source": [
    "### Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "394198b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Training and testing\n",
    "X=[]\n",
    "for sentence in data.data:\n",
    "    sentence_tokenised = word_tokenize(sentence)\n",
    "    X.append(sentence_tokenised)\n",
    "y=data.target\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split (data.data, data.target , test_size=0.2)\n",
    "# print(len(X_train), len(y_train), len(X_test),len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf4c598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: cescript@mtu.edu (Charles Scripter)\\nSubject: Re: Some more about gun control...\\nNntp-Posting-Host: fishlab3.fsh.mtu.edu\\nOrganization: Help, my server\\'s fallen, and can\\'t get up (MTU)\\nX-Newsreader: TIN [version 1.1 PL8]\\nLines: 185\\n\\nIn article <C5Bu9M.2K7@ulowell.ulowell.edu>\\njrutledg@cs.ulowell.edu (John Lawrence Rutledge) wrote:\\n\\n> In article <1q96tpINNpcn@gap.caltech.edu> arc@cco.caltech.edu\\n> (Aaron Ray Clements) writes:\\n> >The Second Amendment is a guarantee of the right to bear arms.  Clearly\\n> >and unequivocally, without infringement.\\n\\n> Unfortunately the Second Amendment is not as clear as you state.  If last \\n> part of it is taken along, it follows what you have said.  The problem\\n> I have is with the first part of the single sentence which makes up the\\n> amendment.  The Second Amendment is:\\n\\n> \\tA well regulated militia, being necessary to the security \\n                         ^^^^^^^ Militia\\n\\n> \\tof a free state, the right of the people to keep and bear \\n                  ^^^^^ State\\n\\n> \\tarms, shall not be infringed.\\n        ^^^^ Arms\\n\\nYou didn\\'t even get the capitalization correct!  Try reading USCA on\\nthe Constitution, or get any other CORRECT version of the\\nConstitution. \\n\\n> This mention of a well regulated militia is what confuses me.  According\\n> to the Federalist Paper\\'s, a well regulated militia has a well defined \\n> structure and follows nationally uniform regulations.\\n\\nPerhaps you should actually READ the Federalist Papers!!\\n\\n    James Madison, Federalist Paper 46: \"Besides the advantage of\\n    being armed, which the Americans possess over the people of almost\\n    every other nation, the existence of subordinate governments, to\\n    which the people are attached, and by which the militia officers\\n    are appointed, forms a barrier against the enterprises of\\n    ambition, more insurmountable than any which a simple government\\n    of any form can admit of.  Notwithstanding the military\\n    establishments in the several kingdoms of Europe, which are\\n    carried as far as the public resources will bear, the governments\\n    are afraid to trust the people with arms.\"\\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n    James Madison, I Annals of Congress 434, 8 June 1789: \"The right\\n    of the people to keep and bear... arms shall not be infringed.  A\\n    well regulated militia, composed of the body of the people,\\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n    trained to arms, is the best and most natural defense of a free\\n    country...\"\\n\\n    Alexander Hamilton, Federalist Paper 29 (on the organization of\\n    the militia): \"Little more can reasonably be aimed at, with\\n    respect to the people at large, than to have them properly armed\\n               ^^^^^^^^^^^^^^^^^^^\\n    and equipped; and in order to see that this be not neglected, it\\n    will be necessary to assemble them once or twice in the course of\\n    a year.\"\\n\\n    Alexander Hamilton, Federalist Paper 29 (speaking of standing\\n    armies): \"... if circumstances should at any time oblige the\\n    government to form an army of any magnitude that army can never be\\n    formidable to the liberties of the people while there is a large\\n    body of citizens, little, if at all, inferior to them in\\n    ^^^^^^^^^^^^^^^^\\n    discipline and the use of arms, who stand ready to defend their\\n                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^*****\\n    own rights and those of their fellow-citizens.\"\\n    ***^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\nBut *surely* Hamilton and Madison didn\\'t mean the PEOPLE when they\\nsaid \"people\", right?  That\\'s why the Amendment refers to \"the Right\\nof the Militia\"?...  ;-)\\n\\n> Your average \\n> 17-45 year old male does not fall into the definition.\\n\\nYou\\'re right, the Militia consists of ALL able bodied males (and\\nprobably females under current interpretation). \\n\\n> Therefore most\\n> members of The Militia, the one the every gun advocate refers to, are\\n> not members of a well organized militia and therefore are not directly\\n\\nThe Amendment does nor refer to \"well organized\", it says \"well\\nregulated\".  I have some targets you may examine if you wish to check\\nhow _well regulated_ I am. \\n\\n> mentioned in the amendment.\\n\\n> If this amendment wanted to allow every member of The Militia to keep\\n> and bear arms, why did it specificly mention a \"well organized militia\" \\n> in the SAME SENTENCE as the right to keep and bear arms?\\n\\nCorrect.  That\\'s why the Right is reserved to the People.  And that\\nwas to insure the People could form a \"well regulated Militia\", not a\\n\"well organized militia\".\\n\\n> It could be\\n> argued that the first part of the sentence is separate from the last \\n> part.  If so, why was it include in the same atomic unit of written\\n\\nWhat do Atomic Units have to do with this argument?  Any moron can set\\nh_bar = C = 1...\\n\\n> instead of a separate sentence?\\n\\nOh, I see what your question is; Why don\\'t you read the federalist\\nPapers?! \\n\\n    James Madison, Federalist Paper 41 (regarding the \"General\\n    Welfare\" clause): \"Nothing is more natural nor common than first\\n    to use a general phrase, and then to explain and qualify it by a\\n    recital of particulars.\"\\n\\nBut what does Madison know about the grammatical style of the 2nd?  He\\nonly wrote it.\\n\\n> The amendment also implies that the right to arms has to due with \\n> the security of a free state.  The Federalist Paper\\'s mention of a\\n> well regulated militia gives many examples of how this militia protects\\n> the security of a free state.  All these examples are actions of a\\n> very organized force, not some John Q. Public with a gun.\\n\\nThat\\'s obviously because you\\'ve never actually *read* the Federalist\\nPapers. \\n\\n> All that the Second Amendment clearly states to me is that the people\\'s\\n> right to form well regulated militias shall not be infringed.  That is \\n> people have the right to join a well organized militia.  This well\\n> organized militia will, of course, provide training in how to use arms\\n> and in basic military tactics.  These training members of the militia\\n> can keep and bear the arms.\\n\\nCan\\'t read, huh?  Show me where the document says \"well organized\\nmilitia\". \\n\\n> Lastly, reading through the Federalist Paper\\'s on well organized \\n> militia it is very clear that many of the reasons for these militias.\\n> One reason stated is the protection from a standing army.  These days\\n> the standing army could easily defeat a group consisting of every \\n> 17-45 year old male and female not in the armied forces.\\n\\nThat is *exactly* why EVERY PERSON should be allowed to own *any*\\nweapon currently in use in the armed forces.\\n\\n> Another\\n> reason stated for well organized militias is to reduced the need\\n> for a standing army.  Well, the US Armied Forces have been a standing\\n> army for more than half the history of the US.\\n\\nBut the major reason is to protect against that very same army.\\n\\n> It seems to me the whole reason for the Second Amendment, to give\\n> the people protection from the US government by guaranteeing that the\\n> people can over through the government if necessary, is a little bit\\n> of an anachronism is this day and age.  Maybe its time to re-think\\n> how this should be done and amend the constitution appropriately.\\n\\n    Abraham Lincoln, First Inaugural Address, March 4, 1861: \"This\\n    country, with its institutions, belongs to the people who inhabit\\n    it.  Whenever they shall grow weary of the existing government,\\n    they can exercise their constitutional right of amending it, or\\n    their revolutionary right to dismember it or overthrow it.\"\\n\\n    Rep. Elbridge Gerry of Massachusetts, spoken during floor debate\\n    over the Second Amendment, I Annals of Congress at 750, 17 August\\n    1789: \"What, Sir, is the use of a militia?  It is to prevent the\\n    establishment of a standing army, the bane of liberty. ...\\n    Whenever Governments mean to invade the rights and liberties of\\n    the people, they always attempt to destroy the militia, in order\\n    to raise an army upon their ruins.\"\\n\\nSo now we know which category Mr. Rutledge is in; He means to destroy\\nour Liberties and Rights.\\n\\n--\\nCharles Scripter   *   cescript@phy.mtu.edu\\nDept of Physics, Michigan Tech, Houghton, MI 49931\\n-------------------------------------------------------------\\n\"...when all government... in little as in great things, shall be\\ndrawn to Washington as the centre of all power, it will render\\npowerless the checks provided of one government on another and will\\nbecome as venal and oppressive as the government from which we\\nseparated.\"   Thomas Jefferson, 1821\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "592463aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5ca64c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Train the word2vec model\n",
    "model = Word2Vec(sentences = X_train, \n",
    "                 vector_size  = 300, \n",
    "                 sg = 0, # sg=0: cbow;  sg=1: skipgram\n",
    "                 window = 5, \n",
    "                 min_count = 2, \n",
    "                 epochs = 30, \n",
    "                 workers = Pool()._processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c7e0bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate aggregated sentence vectors based on the word vectors for each word in the sentence\n",
    "words = set(model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([model.wv[i] for i in ls if i in words])  \n",
    "                         for ls in X_train])\n",
    "X_test_vect = np.array([np.array([model.wv[i] for i in ls if i in words])   \n",
    "                        for ls in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b41e1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.4822696 ,  0.5172229 ,  0.00353569, ...,  0.9546994 ,\n",
      "         -1.560929  ,  0.08340311],\n",
      "        [-0.43393812,  0.06147608,  0.4284799 , ...,  0.23191051,\n",
      "          0.40643778, -0.13344742],\n",
      "        [-0.20166253, -0.26063353,  0.2960698 , ..., -0.04605021,\n",
      "          0.15982087,  0.0581563 ],\n",
      "        ...,\n",
      "        [ 0.20323786, -0.05370469,  0.17035607, ...,  0.21469137,\n",
      "          0.01777811,  0.34219688],\n",
      "        [-0.6061727 , -0.20583513,  0.6784299 , ...,  0.34874967,\n",
      "          0.01030018, -0.42454785],\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977]], dtype=float32)\n",
      " array([[ 1.4822696 ,  0.5172229 ,  0.00353569, ...,  0.9546994 ,\n",
      "         -1.560929  ,  0.08340311],\n",
      "        [-0.43393812,  0.06147608,  0.4284799 , ...,  0.23191051,\n",
      "          0.40643778, -0.13344742],\n",
      "        [-0.20166253, -0.26063353,  0.2960698 , ..., -0.04605021,\n",
      "          0.15982087,  0.0581563 ],\n",
      "        ...,\n",
      "        [-0.73644614, -2.009004  ,  0.08747156, ..., -1.2259071 ,\n",
      "          0.00978301,  0.9114051 ],\n",
      "        [ 0.8772364 , -1.2560716 ,  0.6041513 , ..., -1.8677348 ,\n",
      "          0.5989413 ,  0.5510509 ],\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977]], dtype=float32)              ] [array([[ 1.4822696 ,  0.5172229 ,  0.00353569, ...,  0.9546994 ,\n",
      "         -1.560929  ,  0.08340311],\n",
      "        [-0.43393812,  0.06147608,  0.4284799 , ...,  0.23191051,\n",
      "          0.40643778, -0.13344742],\n",
      "        [-0.20166253, -0.26063353,  0.2960698 , ..., -0.04605021,\n",
      "          0.15982087,  0.0581563 ],\n",
      "        ...,\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977],\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977],\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977]], dtype=float32)\n",
      " array([[ 1.4822696 ,  0.5172229 ,  0.00353569, ...,  0.9546994 ,\n",
      "         -1.560929  ,  0.08340311],\n",
      "        [-0.43393812,  0.06147608,  0.4284799 , ...,  0.23191051,\n",
      "          0.40643778, -0.13344742],\n",
      "        [-0.20166253, -0.26063353,  0.2960698 , ..., -0.04605021,\n",
      "          0.15982087,  0.0581563 ],\n",
      "        ...,\n",
      "        [-0.40772605,  0.34359592, -0.02968098, ..., -1.5175213 ,\n",
      "          0.6831334 , -0.84198177],\n",
      "        [-0.40772605,  0.34359592, -0.02968098, ..., -1.5175213 ,\n",
      "          0.6831334 , -0.84198177],\n",
      "        [-0.54098654,  0.15488732,  0.0456658 , ..., -0.15958712,\n",
      "         -0.15108152,  0.25465977]], dtype=float32)              ]\n"
     ]
    }
   ],
   "source": [
    "# print(len(X_train_vect), len(X_test_vect))\n",
    "print(X_train_vect[:2], X_test_vect[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff986204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6692ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= {\n",
    "#          'nb':MultinomialNB(),\n",
    "         'lrWord2Vec': LogisticRegression(),\n",
    "#          'rf': RandomForestClassifier( random_state=49), \n",
    "         'dtcWord2Vec': DecisionTreeClassifier(),\n",
    "#          'svc': SVC(),\n",
    "#          'sgdword2Vec': SGDClassifier(tol=1e-3),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a181170",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_parameter_grid = {\n",
    "                           'lrWord2Vec':{\n",
    "                                          \"lrWord2Vec__penalty\": [ 'none', 'l2', ], # 'none', 'elasticnet'\n",
    "                                          \"lrWord2Vec__C\": [0.001, 0.01, 0.1, 0.5, 1, 1.5],\n",
    "                                          # \"lr__solver\": ['newton-cg', 'lbfgs', 'liblinear'], #'sag', 'saga'],\n",
    "                                          \"lrWord2Vec__max_iter\": [500],\n",
    "                                          \"lrWord2Vec__multi_class\": ['auto'],\n",
    "                                          \"lrWord2Vec__n_jobs\": [-1],\n",
    "                                          # \"lr__l1_ratio\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "                        },\n",
    "                           'dt': {\n",
    "                                           \"dt__min_samples_split\":[2,4],\n",
    "                            \n",
    "                        },\n",
    "#                         'rf': {\n",
    "#                                # \"rf__criterion\": [ 'gini', 'entropy', 'log_loss'],\n",
    "#                                # \"rf__n_estimators\": [20, 50, 100],\n",
    "#                                \"rf__max_depth\": [None, 2, 4, 6],\n",
    "#                                # \"rf__min_samples_split\": [2,5, 10, 15, 20, 30, 50],\n",
    "#                               },\n",
    "#                         'nb': {  \n",
    "#                         },\n",
    "#                         'svc': {\n",
    "#                                 \"svc__C\": [ 0, 0.5],\n",
    "#                                 # \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#                                 # \"svc__gamma\": ['scale', 'auto'],\n",
    "#                                 #\"svc__shrinking\": [True, False],\n",
    "#                                }\n",
    "                          'dtcWord2Vec': {\n",
    "                                           \"dtcWord2Vec__min_samples_split\":[2,4],\n",
    "                            \n",
    "                        },\n",
    "#                         'sgdword2Vec': {\n",
    "#                                 'sgdword2Vec__max_iter': (20,),\n",
    "#                                 'sgdword2Vec__alpha': (0.00001, 0.000001),\n",
    "#                                 'sgd_word2Vec__penalty': ('l2', 'elasticnet'),\n",
    "#                         },\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "277ff85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Processing 2 models. Please, wait...\n",
      "\n",
      "Algorithm being processed: lrWord2Vec\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "Best score: 0.820 \n",
      "\n",
      "\n",
      "Algorithm being processed: dtcWord2Vec\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score: 0.710 \n",
      "\n",
      "[+] Finish Processing\n",
      "\n",
      "CPU times: total: 1.17 s\n",
      "Wall time: 5.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_dict={}\n",
    "cv = 2            # Cross validation\n",
    "\n",
    "comparison_matrix = pd.DataFrame(columns=['Model', \n",
    "                                          'Pipeline', \n",
    "                                          'Best_Estimator',\n",
    "                                          'Best_Accuracy'])\n",
    "\n",
    "print(\"[+] Processing {} models. Please, wait...\".format(len(models)))\n",
    "\n",
    "for model in models:\n",
    "    print(\"\\nAlgorithm being processed: {}\".format(model))\n",
    "    pipeline = Pipeline([(model, models[model])])  \n",
    "    grid_search_pipe = GridSearchCV(pipeline, \n",
    "                                param_grid=word2vec_parameter_grid[model], \n",
    "                                cv=cv,\n",
    "                                n_jobs=-1, \n",
    "                                verbose=1)\n",
    "    grid_search_pipe.fit(X_train_vect_avg, y_train)\n",
    "\n",
    "    print(\"Best score: %0.3f \\n\" % grid_search_pipe.best_score_)\n",
    "#     print(\"Best Parameters: {} \\n\".format( grid_search_pipe.best_estimator_.get_params()))\n",
    "    best_parameters = grid_search_pipe.best_estimator_.get_params()\n",
    "    #print(\"Best estimator: {} \\n\".format( grid_search_pipe.best_estimator_))\n",
    "    series_aux =pd.Series(data=[model,\n",
    "                                models[model],\n",
    "                                grid_search_pipe.best_estimator_,\n",
    "                                grid_search_pipe.best_score_],\n",
    "                          index=comparison_matrix.columns)\n",
    "    \n",
    "\n",
    "        \n",
    "    comparison_matrix = comparison_matrix.append(series_aux, \n",
    "                                                 ignore_index=True)\n",
    "\n",
    "print(\"[+] Finish Processing\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14ad6d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a8a7c4",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef24fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import doc2vec\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the complaint narrative.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d24193cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d736db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['From:', 'cescript@mtu.edu', '(Charles', 'Scripter)', 'Subject:', 'Re:', 'Some', 'more', 'about', 'gun', 'control...', 'Nntp-Posting-Host:', 'fishlab3.fsh.mtu.edu', 'Organization:', 'Help,', 'my', \"server's\", 'fallen,', 'and', \"can't\", 'get', 'up', '(MTU)', 'X-Newsreader:', 'TIN', '[version', '1.1', 'PL8]', 'Lines:', '185', 'In', 'article', '<C5Bu9M.2K7@ulowell.ulowell.edu>', 'jrutledg@cs.ulowell.edu', '(John', 'Lawrence', 'Rutledge)', 'wrote:', '>', 'In', 'article', '<1q96tpINNpcn@gap.caltech.edu>', 'arc@cco.caltech.edu', '>', '(Aaron', 'Ray', 'Clements)', 'writes:', '>', '>The', 'Second', 'Amendment', 'is', 'a', 'guarantee', 'of', 'the', 'right', 'to', 'bear', 'arms.', 'Clearly', '>', '>and', 'unequivocally,', 'without', 'infringement.', '>', 'Unfortunately', 'the', 'Second', 'Amendment', 'is', 'not', 'as', 'clear', 'as', 'you', 'state.', 'If', 'last', '>', 'part', 'of', 'it', 'is', 'taken', 'along,', 'it', 'follows', 'what', 'you', 'have', 'said.', 'The', 'problem', '>', 'I', 'have', 'is', 'with', 'the', 'first', 'part', 'of', 'the', 'single', 'sentence', 'which', 'makes', 'up', 'the', '>', 'amendment.', 'The', 'Second', 'Amendment', 'is:', '>', 'A', 'well', 'regulated', 'militia,', 'being', 'necessary', 'to', 'the', 'security', '^^^^^^^', 'Militia', '>', 'of', 'a', 'free', 'state,', 'the', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear', '^^^^^', 'State', '>', 'arms,', 'shall', 'not', 'be', 'infringed.', '^^^^', 'Arms', 'You', \"didn't\", 'even', 'get', 'the', 'capitalization', 'correct!', 'Try', 'reading', 'USCA', 'on', 'the', 'Constitution,', 'or', 'get', 'any', 'other', 'CORRECT', 'version', 'of', 'the', 'Constitution.', '>', 'This', 'mention', 'of', 'a', 'well', 'regulated', 'militia', 'is', 'what', 'confuses', 'me.', 'According', '>', 'to', 'the', 'Federalist', \"Paper's,\", 'a', 'well', 'regulated', 'militia', 'has', 'a', 'well', 'defined', '>', 'structure', 'and', 'follows', 'nationally', 'uniform', 'regulations.', 'Perhaps', 'you', 'should', 'actually', 'READ', 'the', 'Federalist', 'Papers!!', 'James', 'Madison,', 'Federalist', 'Paper', '46:', '\"Besides', 'the', 'advantage', 'of', 'being', 'armed,', 'which', 'the', 'Americans', 'possess', 'over', 'the', 'people', 'of', 'almost', 'every', 'other', 'nation,', 'the', 'existence', 'of', 'subordinate', 'governments,', 'to', 'which', 'the', 'people', 'are', 'attached,', 'and', 'by', 'which', 'the', 'militia', 'officers', 'are', 'appointed,', 'forms', 'a', 'barrier', 'against', 'the', 'enterprises', 'of', 'ambition,', 'more', 'insurmountable', 'than', 'any', 'which', 'a', 'simple', 'government', 'of', 'any', 'form', 'can', 'admit', 'of.', 'Notwithstanding', 'the', 'military', 'establishments', 'in', 'the', 'several', 'kingdoms', 'of', 'Europe,', 'which', 'are', 'carried', 'as', 'far', 'as', 'the', 'public', 'resources', 'will', 'bear,', 'the', 'governments', 'are', 'afraid', 'to', 'trust', 'the', 'people', 'with', 'arms.\"', '^^^^^^^^^^^^^^^^^^^^^^^^^^', 'James', 'Madison,', 'I', 'Annals', 'of', 'Congress', '434,', '8', 'June', '1789:', '\"The', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear...', 'arms', 'shall', 'not', 'be', 'infringed.', 'A', 'well', 'regulated', 'militia,', 'composed', 'of', 'the', 'body', 'of', 'the', 'people,', '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', 'trained', 'to', 'arms,', 'is', 'the', 'best', 'and', 'most', 'natural', 'defense', 'of', 'a', 'free', 'country...\"', 'Alexander', 'Hamilton,', 'Federalist', 'Paper', '29', '(on', 'the', 'organization', 'of', 'the', 'militia):', '\"Little', 'more', 'can', 'reasonably', 'be', 'aimed', 'at,', 'with', 'respect', 'to', 'the', 'people', 'at', 'large,', 'than', 'to', 'have', 'them', 'properly', 'armed', '^^^^^^^^^^^^^^^^^^^', 'and', 'equipped;', 'and', 'in', 'order', 'to', 'see', 'that', 'this', 'be', 'not', 'neglected,', 'it', 'will', 'be', 'necessary', 'to', 'assemble', 'them', 'once', 'or', 'twice', 'in', 'the', 'course', 'of', 'a', 'year.\"', 'Alexander', 'Hamilton,', 'Federalist', 'Paper', '29', '(speaking', 'of', 'standing', 'armies):', '\"...', 'if', 'circumstances', 'should', 'at', 'any', 'time', 'oblige', 'the', 'government', 'to', 'form', 'an', 'army', 'of', 'any', 'magnitude', 'that', 'army', 'can', 'never', 'be', 'formidable', 'to', 'the', 'liberties', 'of', 'the', 'people', 'while', 'there', 'is', 'a', 'large', 'body', 'of', 'citizens,', 'little,', 'if', 'at', 'all,', 'inferior', 'to', 'them', 'in', '^^^^^^^^^^^^^^^^', 'discipline', 'and', 'the', 'use', 'of', 'arms,', 'who', 'stand', 'ready', 'to', 'defend', 'their', '^^^^^^^^^^^^^^^^^^^^^^^^^^*****', 'own', 'rights', 'and', 'those', 'of', 'their', 'fellow-citizens.\"', '***^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', 'But', '*surely*', 'Hamilton', 'and', 'Madison', \"didn't\", 'mean', 'the', 'PEOPLE', 'when', 'they', 'said', '\"people\",', 'right?', \"That's\", 'why', 'the', 'Amendment', 'refers', 'to', '\"the', 'Right', 'of', 'the', 'Militia\"?...', ';-)', '>', 'Your', 'average', '>', '17-45', 'year', 'old', 'male', 'does', 'not', 'fall', 'into', 'the', 'definition.', \"You're\", 'right,', 'the', 'Militia', 'consists', 'of', 'ALL', 'able', 'bodied', 'males', '(and', 'probably', 'females', 'under', 'current', 'interpretation).', '>', 'Therefore', 'most', '>', 'members', 'of', 'The', 'Militia,', 'the', 'one', 'the', 'every', 'gun', 'advocate', 'refers', 'to,', 'are', '>', 'not', 'members', 'of', 'a', 'well', 'organized', 'militia', 'and', 'therefore', 'are', 'not', 'directly', 'The', 'Amendment', 'does', 'nor', 'refer', 'to', '\"well', 'organized\",', 'it', 'says', '\"well', 'regulated\".', 'I', 'have', 'some', 'targets', 'you', 'may', 'examine', 'if', 'you', 'wish', 'to', 'check', 'how', '_well', 'regulated_', 'I', 'am.', '>', 'mentioned', 'in', 'the', 'amendment.', '>', 'If', 'this', 'amendment', 'wanted', 'to', 'allow', 'every', 'member', 'of', 'The', 'Militia', 'to', 'keep', '>', 'and', 'bear', 'arms,', 'why', 'did', 'it', 'specificly', 'mention', 'a', '\"well', 'organized', 'militia\"', '>', 'in', 'the', 'SAME', 'SENTENCE', 'as', 'the', 'right', 'to', 'keep', 'and', 'bear', 'arms?', 'Correct.', \"That's\", 'why', 'the', 'Right', 'is', 'reserved', 'to', 'the', 'People.', 'And', 'that', 'was', 'to', 'insure', 'the', 'People', 'could', 'form', 'a', '\"well', 'regulated', 'Militia\",', 'not', 'a', '\"well', 'organized', 'militia\".', '>', 'It', 'could', 'be', '>', 'argued', 'that', 'the', 'first', 'part', 'of', 'the', 'sentence', 'is', 'separate', 'from', 'the', 'last', '>', 'part.', 'If', 'so,', 'why', 'was', 'it', 'include', 'in', 'the', 'same', 'atomic', 'unit', 'of', 'written', 'What', 'do', 'Atomic', 'Units', 'have', 'to', 'do', 'with', 'this', 'argument?', 'Any', 'moron', 'can', 'set', 'h_bar', '=', 'C', '=', '1...', '>', 'instead', 'of', 'a', 'separate', 'sentence?', 'Oh,', 'I', 'see', 'what', 'your', 'question', 'is;', 'Why', \"don't\", 'you', 'read', 'the', 'federalist', 'Papers?!', 'James', 'Madison,', 'Federalist', 'Paper', '41', '(regarding', 'the', '\"General', 'Welfare\"', 'clause):', '\"Nothing', 'is', 'more', 'natural', 'nor', 'common', 'than', 'first', 'to', 'use', 'a', 'general', 'phrase,', 'and', 'then', 'to', 'explain', 'and', 'qualify', 'it', 'by', 'a', 'recital', 'of', 'particulars.\"', 'But', 'what', 'does', 'Madison', 'know', 'about', 'the', 'grammatical', 'style', 'of', 'the', '2nd?', 'He', 'only', 'wrote', 'it.', '>', 'The', 'amendment', 'also', 'implies', 'that', 'the', 'right', 'to', 'arms', 'has', 'to', 'due', 'with', '>', 'the', 'security', 'of', 'a', 'free', 'state.', 'The', 'Federalist', \"Paper's\", 'mention', 'of', 'a', '>', 'well', 'regulated', 'militia', 'gives', 'many', 'examples', 'of', 'how', 'this', 'militia', 'protects', '>', 'the', 'security', 'of', 'a', 'free', 'state.', 'All', 'these', 'examples', 'are', 'actions', 'of', 'a', '>', 'very', 'organized', 'force,', 'not', 'some', 'John', 'Q.', 'Public', 'with', 'a', 'gun.', \"That's\", 'obviously', 'because', \"you've\", 'never', 'actually', '*read*', 'the', 'Federalist', 'Papers.', '>', 'All', 'that', 'the', 'Second', 'Amendment', 'clearly', 'states', 'to', 'me', 'is', 'that', 'the', \"people's\", '>', 'right', 'to', 'form', 'well', 'regulated', 'militias', 'shall', 'not', 'be', 'infringed.', 'That', 'is', '>', 'people', 'have', 'the', 'right', 'to', 'join', 'a', 'well', 'organized', 'militia.', 'This', 'well', '>', 'organized', 'militia', 'will,', 'of', 'course,', 'provide', 'training', 'in', 'how', 'to', 'use', 'arms', '>', 'and', 'in', 'basic', 'military', 'tactics.', 'These', 'training', 'members', 'of', 'the', 'militia', '>', 'can', 'keep', 'and', 'bear', 'the', 'arms.', \"Can't\", 'read,', 'huh?', 'Show', 'me', 'where', 'the', 'document', 'says', '\"well', 'organized', 'militia\".', '>', 'Lastly,', 'reading', 'through', 'the', 'Federalist', \"Paper's\", 'on', 'well', 'organized', '>', 'militia', 'it', 'is', 'very', 'clear', 'that', 'many', 'of', 'the', 'reasons', 'for', 'these', 'militias.', '>', 'One', 'reason', 'stated', 'is', 'the', 'protection', 'from', 'a', 'standing', 'army.', 'These', 'days', '>', 'the', 'standing', 'army', 'could', 'easily', 'defeat', 'a', 'group', 'consisting', 'of', 'every', '>', '17-45', 'year', 'old', 'male', 'and', 'female', 'not', 'in', 'the', 'armied', 'forces.', 'That', 'is', '*exactly*', 'why', 'EVERY', 'PERSON', 'should', 'be', 'allowed', 'to', 'own', '*any*', 'weapon', 'currently', 'in', 'use', 'in', 'the', 'armed', 'forces.', '>', 'Another', '>', 'reason', 'stated', 'for', 'well', 'organized', 'militias', 'is', 'to', 'reduced', 'the', 'need', '>', 'for', 'a', 'standing', 'army.', 'Well,', 'the', 'US', 'Armied', 'Forces', 'have', 'been', 'a', 'standing', '>', 'army', 'for', 'more', 'than', 'half', 'the', 'history', 'of', 'the', 'US.', 'But', 'the', 'major', 'reason', 'is', 'to', 'protect', 'against', 'that', 'very', 'same', 'army.', '>', 'It', 'seems', 'to', 'me', 'the', 'whole', 'reason', 'for', 'the', 'Second', 'Amendment,', 'to', 'give', '>', 'the', 'people', 'protection', 'from', 'the', 'US', 'government', 'by', 'guaranteeing', 'that', 'the', '>', 'people', 'can', 'over', 'through', 'the', 'government', 'if', 'necessary,', 'is', 'a', 'little', 'bit', '>', 'of', 'an', 'anachronism', 'is', 'this', 'day', 'and', 'age.', 'Maybe', 'its', 'time', 'to', 're-think', '>', 'how', 'this', 'should', 'be', 'done', 'and', 'amend', 'the', 'constitution', 'appropriately.', 'Abraham', 'Lincoln,', 'First', 'Inaugural', 'Address,', 'March', '4,', '1861:', '\"This', 'country,', 'with', 'its', 'institutions,', 'belongs', 'to', 'the', 'people', 'who', 'inhabit', 'it.', 'Whenever', 'they', 'shall', 'grow', 'weary', 'of', 'the', 'existing', 'government,', 'they', 'can', 'exercise', 'their', 'constitutional', 'right', 'of', 'amending', 'it,', 'or', 'their', 'revolutionary', 'right', 'to', 'dismember', 'it', 'or', 'overthrow', 'it.\"', 'Rep.', 'Elbridge', 'Gerry', 'of', 'Massachusetts,', 'spoken', 'during', 'floor', 'debate', 'over', 'the', 'Second', 'Amendment,', 'I', 'Annals', 'of', 'Congress', 'at', '750,', '17', 'August', '1789:', '\"What,', 'Sir,', 'is', 'the', 'use', 'of', 'a', 'militia?', 'It', 'is', 'to', 'prevent', 'the', 'establishment', 'of', 'a', 'standing', 'army,', 'the', 'bane', 'of', 'liberty.', '...', 'Whenever', 'Governments', 'mean', 'to', 'invade', 'the', 'rights', 'and', 'liberties', 'of', 'the', 'people,', 'they', 'always', 'attempt', 'to', 'destroy', 'the', 'militia,', 'in', 'order', 'to', 'raise', 'an', 'army', 'upon', 'their', 'ruins.\"', 'So', 'now', 'we', 'know', 'which', 'category', 'Mr.', 'Rutledge', 'is', 'in;', 'He', 'means', 'to', 'destroy', 'our', 'Liberties', 'and', 'Rights.', '--', 'Charles', 'Scripter', '*', 'cescript@phy.mtu.edu', 'Dept', 'of', 'Physics,', 'Michigan', 'Tech,', 'Houghton,', 'MI', '49931', '-------------------------------------------------------------', '\"...when', 'all', 'government...', 'in', 'little', 'as', 'in', 'great', 'things,', 'shall', 'be', 'drawn', 'to', 'Washington', 'as', 'the', 'centre', 'of', 'all', 'power,', 'it', 'will', 'render', 'powerless', 'the', 'checks', 'provided', 'of', 'one', 'government', 'on', 'another', 'and', 'will', 'become', 'as', 'venal', 'and', 'oppressive', 'as', 'the', 'government', 'from', 'which', 'we', 'separated.\"', 'Thomas', 'Jefferson,', '1821'], tags=['Train_1'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1f73e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7fc50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['From:', 'chuck@eng.umd.edu', '(Chuck', 'Harris', '-', 'WA3UQV)', 'Subject:', 'Re:', 'CNN', 'for', 'sale', 'Organization:', 'University', 'of', 'Maryland,', 'Department', 'of', 'Electrical', 'Engineering', 'Lines:', '11', 'Distribution:', 'usa', 'NNTP-Posting-Host:', 'bree.eng.umd.edu', 'In', 'article', '<C5soMx.HMD@boi.hp.com>', 'kde@boi.hp.com', '(Keith', 'Emmen)', 'writes:', '>If', 'anyone', 'is', 'keeping', 'a', 'list', 'of', 'the', 'potential', 'contributors,', '>you', 'can', 'put', 'me', 'down', 'for', '$1000.00', 'under', 'the', 'conditions', 'above', 'Seems', 'to', 'me', 'folks,', 'that', 'if', 'you', 'are', 'so', 'interested', 'in', 'acquiring', 'CNN,', 'just', 'buy', 'your', '$1000', 'worth', 'of', 'stock', 'today.', \"It's\", 'being', 'traded', 'everyday.', 'After', 'you', 'own', 'your', 'piece,', 'we', 'can', 'work', 'on', 'the', 'proxy', 'votes', 'later.', \"It's\", 'probably', 'even', 'a', 'good', 'investment.', 'Chuck', 'Harris', '-', 'WA3UQV', 'chuck@eng.umd.edu'], tags=['Train_0']),\n",
       " TaggedDocument(words=['From:', 'cescript@mtu.edu', '(Charles', 'Scripter)', 'Subject:', 'Re:', 'Some', 'more', 'about', 'gun', 'control...', 'Nntp-Posting-Host:', 'fishlab3.fsh.mtu.edu', 'Organization:', 'Help,', 'my', \"server's\", 'fallen,', 'and', \"can't\", 'get', 'up', '(MTU)', 'X-Newsreader:', 'TIN', '[version', '1.1', 'PL8]', 'Lines:', '185', 'In', 'article', '<C5Bu9M.2K7@ulowell.ulowell.edu>', 'jrutledg@cs.ulowell.edu', '(John', 'Lawrence', 'Rutledge)', 'wrote:', '>', 'In', 'article', '<1q96tpINNpcn@gap.caltech.edu>', 'arc@cco.caltech.edu', '>', '(Aaron', 'Ray', 'Clements)', 'writes:', '>', '>The', 'Second', 'Amendment', 'is', 'a', 'guarantee', 'of', 'the', 'right', 'to', 'bear', 'arms.', 'Clearly', '>', '>and', 'unequivocally,', 'without', 'infringement.', '>', 'Unfortunately', 'the', 'Second', 'Amendment', 'is', 'not', 'as', 'clear', 'as', 'you', 'state.', 'If', 'last', '>', 'part', 'of', 'it', 'is', 'taken', 'along,', 'it', 'follows', 'what', 'you', 'have', 'said.', 'The', 'problem', '>', 'I', 'have', 'is', 'with', 'the', 'first', 'part', 'of', 'the', 'single', 'sentence', 'which', 'makes', 'up', 'the', '>', 'amendment.', 'The', 'Second', 'Amendment', 'is:', '>', 'A', 'well', 'regulated', 'militia,', 'being', 'necessary', 'to', 'the', 'security', '^^^^^^^', 'Militia', '>', 'of', 'a', 'free', 'state,', 'the', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear', '^^^^^', 'State', '>', 'arms,', 'shall', 'not', 'be', 'infringed.', '^^^^', 'Arms', 'You', \"didn't\", 'even', 'get', 'the', 'capitalization', 'correct!', 'Try', 'reading', 'USCA', 'on', 'the', 'Constitution,', 'or', 'get', 'any', 'other', 'CORRECT', 'version', 'of', 'the', 'Constitution.', '>', 'This', 'mention', 'of', 'a', 'well', 'regulated', 'militia', 'is', 'what', 'confuses', 'me.', 'According', '>', 'to', 'the', 'Federalist', \"Paper's,\", 'a', 'well', 'regulated', 'militia', 'has', 'a', 'well', 'defined', '>', 'structure', 'and', 'follows', 'nationally', 'uniform', 'regulations.', 'Perhaps', 'you', 'should', 'actually', 'READ', 'the', 'Federalist', 'Papers!!', 'James', 'Madison,', 'Federalist', 'Paper', '46:', '\"Besides', 'the', 'advantage', 'of', 'being', 'armed,', 'which', 'the', 'Americans', 'possess', 'over', 'the', 'people', 'of', 'almost', 'every', 'other', 'nation,', 'the', 'existence', 'of', 'subordinate', 'governments,', 'to', 'which', 'the', 'people', 'are', 'attached,', 'and', 'by', 'which', 'the', 'militia', 'officers', 'are', 'appointed,', 'forms', 'a', 'barrier', 'against', 'the', 'enterprises', 'of', 'ambition,', 'more', 'insurmountable', 'than', 'any', 'which', 'a', 'simple', 'government', 'of', 'any', 'form', 'can', 'admit', 'of.', 'Notwithstanding', 'the', 'military', 'establishments', 'in', 'the', 'several', 'kingdoms', 'of', 'Europe,', 'which', 'are', 'carried', 'as', 'far', 'as', 'the', 'public', 'resources', 'will', 'bear,', 'the', 'governments', 'are', 'afraid', 'to', 'trust', 'the', 'people', 'with', 'arms.\"', '^^^^^^^^^^^^^^^^^^^^^^^^^^', 'James', 'Madison,', 'I', 'Annals', 'of', 'Congress', '434,', '8', 'June', '1789:', '\"The', 'right', 'of', 'the', 'people', 'to', 'keep', 'and', 'bear...', 'arms', 'shall', 'not', 'be', 'infringed.', 'A', 'well', 'regulated', 'militia,', 'composed', 'of', 'the', 'body', 'of', 'the', 'people,', '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', 'trained', 'to', 'arms,', 'is', 'the', 'best', 'and', 'most', 'natural', 'defense', 'of', 'a', 'free', 'country...\"', 'Alexander', 'Hamilton,', 'Federalist', 'Paper', '29', '(on', 'the', 'organization', 'of', 'the', 'militia):', '\"Little', 'more', 'can', 'reasonably', 'be', 'aimed', 'at,', 'with', 'respect', 'to', 'the', 'people', 'at', 'large,', 'than', 'to', 'have', 'them', 'properly', 'armed', '^^^^^^^^^^^^^^^^^^^', 'and', 'equipped;', 'and', 'in', 'order', 'to', 'see', 'that', 'this', 'be', 'not', 'neglected,', 'it', 'will', 'be', 'necessary', 'to', 'assemble', 'them', 'once', 'or', 'twice', 'in', 'the', 'course', 'of', 'a', 'year.\"', 'Alexander', 'Hamilton,', 'Federalist', 'Paper', '29', '(speaking', 'of', 'standing', 'armies):', '\"...', 'if', 'circumstances', 'should', 'at', 'any', 'time', 'oblige', 'the', 'government', 'to', 'form', 'an', 'army', 'of', 'any', 'magnitude', 'that', 'army', 'can', 'never', 'be', 'formidable', 'to', 'the', 'liberties', 'of', 'the', 'people', 'while', 'there', 'is', 'a', 'large', 'body', 'of', 'citizens,', 'little,', 'if', 'at', 'all,', 'inferior', 'to', 'them', 'in', '^^^^^^^^^^^^^^^^', 'discipline', 'and', 'the', 'use', 'of', 'arms,', 'who', 'stand', 'ready', 'to', 'defend', 'their', '^^^^^^^^^^^^^^^^^^^^^^^^^^*****', 'own', 'rights', 'and', 'those', 'of', 'their', 'fellow-citizens.\"', '***^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^', 'But', '*surely*', 'Hamilton', 'and', 'Madison', \"didn't\", 'mean', 'the', 'PEOPLE', 'when', 'they', 'said', '\"people\",', 'right?', \"That's\", 'why', 'the', 'Amendment', 'refers', 'to', '\"the', 'Right', 'of', 'the', 'Militia\"?...', ';-)', '>', 'Your', 'average', '>', '17-45', 'year', 'old', 'male', 'does', 'not', 'fall', 'into', 'the', 'definition.', \"You're\", 'right,', 'the', 'Militia', 'consists', 'of', 'ALL', 'able', 'bodied', 'males', '(and', 'probably', 'females', 'under', 'current', 'interpretation).', '>', 'Therefore', 'most', '>', 'members', 'of', 'The', 'Militia,', 'the', 'one', 'the', 'every', 'gun', 'advocate', 'refers', 'to,', 'are', '>', 'not', 'members', 'of', 'a', 'well', 'organized', 'militia', 'and', 'therefore', 'are', 'not', 'directly', 'The', 'Amendment', 'does', 'nor', 'refer', 'to', '\"well', 'organized\",', 'it', 'says', '\"well', 'regulated\".', 'I', 'have', 'some', 'targets', 'you', 'may', 'examine', 'if', 'you', 'wish', 'to', 'check', 'how', '_well', 'regulated_', 'I', 'am.', '>', 'mentioned', 'in', 'the', 'amendment.', '>', 'If', 'this', 'amendment', 'wanted', 'to', 'allow', 'every', 'member', 'of', 'The', 'Militia', 'to', 'keep', '>', 'and', 'bear', 'arms,', 'why', 'did', 'it', 'specificly', 'mention', 'a', '\"well', 'organized', 'militia\"', '>', 'in', 'the', 'SAME', 'SENTENCE', 'as', 'the', 'right', 'to', 'keep', 'and', 'bear', 'arms?', 'Correct.', \"That's\", 'why', 'the', 'Right', 'is', 'reserved', 'to', 'the', 'People.', 'And', 'that', 'was', 'to', 'insure', 'the', 'People', 'could', 'form', 'a', '\"well', 'regulated', 'Militia\",', 'not', 'a', '\"well', 'organized', 'militia\".', '>', 'It', 'could', 'be', '>', 'argued', 'that', 'the', 'first', 'part', 'of', 'the', 'sentence', 'is', 'separate', 'from', 'the', 'last', '>', 'part.', 'If', 'so,', 'why', 'was', 'it', 'include', 'in', 'the', 'same', 'atomic', 'unit', 'of', 'written', 'What', 'do', 'Atomic', 'Units', 'have', 'to', 'do', 'with', 'this', 'argument?', 'Any', 'moron', 'can', 'set', 'h_bar', '=', 'C', '=', '1...', '>', 'instead', 'of', 'a', 'separate', 'sentence?', 'Oh,', 'I', 'see', 'what', 'your', 'question', 'is;', 'Why', \"don't\", 'you', 'read', 'the', 'federalist', 'Papers?!', 'James', 'Madison,', 'Federalist', 'Paper', '41', '(regarding', 'the', '\"General', 'Welfare\"', 'clause):', '\"Nothing', 'is', 'more', 'natural', 'nor', 'common', 'than', 'first', 'to', 'use', 'a', 'general', 'phrase,', 'and', 'then', 'to', 'explain', 'and', 'qualify', 'it', 'by', 'a', 'recital', 'of', 'particulars.\"', 'But', 'what', 'does', 'Madison', 'know', 'about', 'the', 'grammatical', 'style', 'of', 'the', '2nd?', 'He', 'only', 'wrote', 'it.', '>', 'The', 'amendment', 'also', 'implies', 'that', 'the', 'right', 'to', 'arms', 'has', 'to', 'due', 'with', '>', 'the', 'security', 'of', 'a', 'free', 'state.', 'The', 'Federalist', \"Paper's\", 'mention', 'of', 'a', '>', 'well', 'regulated', 'militia', 'gives', 'many', 'examples', 'of', 'how', 'this', 'militia', 'protects', '>', 'the', 'security', 'of', 'a', 'free', 'state.', 'All', 'these', 'examples', 'are', 'actions', 'of', 'a', '>', 'very', 'organized', 'force,', 'not', 'some', 'John', 'Q.', 'Public', 'with', 'a', 'gun.', \"That's\", 'obviously', 'because', \"you've\", 'never', 'actually', '*read*', 'the', 'Federalist', 'Papers.', '>', 'All', 'that', 'the', 'Second', 'Amendment', 'clearly', 'states', 'to', 'me', 'is', 'that', 'the', \"people's\", '>', 'right', 'to', 'form', 'well', 'regulated', 'militias', 'shall', 'not', 'be', 'infringed.', 'That', 'is', '>', 'people', 'have', 'the', 'right', 'to', 'join', 'a', 'well', 'organized', 'militia.', 'This', 'well', '>', 'organized', 'militia', 'will,', 'of', 'course,', 'provide', 'training', 'in', 'how', 'to', 'use', 'arms', '>', 'and', 'in', 'basic', 'military', 'tactics.', 'These', 'training', 'members', 'of', 'the', 'militia', '>', 'can', 'keep', 'and', 'bear', 'the', 'arms.', \"Can't\", 'read,', 'huh?', 'Show', 'me', 'where', 'the', 'document', 'says', '\"well', 'organized', 'militia\".', '>', 'Lastly,', 'reading', 'through', 'the', 'Federalist', \"Paper's\", 'on', 'well', 'organized', '>', 'militia', 'it', 'is', 'very', 'clear', 'that', 'many', 'of', 'the', 'reasons', 'for', 'these', 'militias.', '>', 'One', 'reason', 'stated', 'is', 'the', 'protection', 'from', 'a', 'standing', 'army.', 'These', 'days', '>', 'the', 'standing', 'army', 'could', 'easily', 'defeat', 'a', 'group', 'consisting', 'of', 'every', '>', '17-45', 'year', 'old', 'male', 'and', 'female', 'not', 'in', 'the', 'armied', 'forces.', 'That', 'is', '*exactly*', 'why', 'EVERY', 'PERSON', 'should', 'be', 'allowed', 'to', 'own', '*any*', 'weapon', 'currently', 'in', 'use', 'in', 'the', 'armed', 'forces.', '>', 'Another', '>', 'reason', 'stated', 'for', 'well', 'organized', 'militias', 'is', 'to', 'reduced', 'the', 'need', '>', 'for', 'a', 'standing', 'army.', 'Well,', 'the', 'US', 'Armied', 'Forces', 'have', 'been', 'a', 'standing', '>', 'army', 'for', 'more', 'than', 'half', 'the', 'history', 'of', 'the', 'US.', 'But', 'the', 'major', 'reason', 'is', 'to', 'protect', 'against', 'that', 'very', 'same', 'army.', '>', 'It', 'seems', 'to', 'me', 'the', 'whole', 'reason', 'for', 'the', 'Second', 'Amendment,', 'to', 'give', '>', 'the', 'people', 'protection', 'from', 'the', 'US', 'government', 'by', 'guaranteeing', 'that', 'the', '>', 'people', 'can', 'over', 'through', 'the', 'government', 'if', 'necessary,', 'is', 'a', 'little', 'bit', '>', 'of', 'an', 'anachronism', 'is', 'this', 'day', 'and', 'age.', 'Maybe', 'its', 'time', 'to', 're-think', '>', 'how', 'this', 'should', 'be', 'done', 'and', 'amend', 'the', 'constitution', 'appropriately.', 'Abraham', 'Lincoln,', 'First', 'Inaugural', 'Address,', 'March', '4,', '1861:', '\"This', 'country,', 'with', 'its', 'institutions,', 'belongs', 'to', 'the', 'people', 'who', 'inhabit', 'it.', 'Whenever', 'they', 'shall', 'grow', 'weary', 'of', 'the', 'existing', 'government,', 'they', 'can', 'exercise', 'their', 'constitutional', 'right', 'of', 'amending', 'it,', 'or', 'their', 'revolutionary', 'right', 'to', 'dismember', 'it', 'or', 'overthrow', 'it.\"', 'Rep.', 'Elbridge', 'Gerry', 'of', 'Massachusetts,', 'spoken', 'during', 'floor', 'debate', 'over', 'the', 'Second', 'Amendment,', 'I', 'Annals', 'of', 'Congress', 'at', '750,', '17', 'August', '1789:', '\"What,', 'Sir,', 'is', 'the', 'use', 'of', 'a', 'militia?', 'It', 'is', 'to', 'prevent', 'the', 'establishment', 'of', 'a', 'standing', 'army,', 'the', 'bane', 'of', 'liberty.', '...', 'Whenever', 'Governments', 'mean', 'to', 'invade', 'the', 'rights', 'and', 'liberties', 'of', 'the', 'people,', 'they', 'always', 'attempt', 'to', 'destroy', 'the', 'militia,', 'in', 'order', 'to', 'raise', 'an', 'army', 'upon', 'their', 'ruins.\"', 'So', 'now', 'we', 'know', 'which', 'category', 'Mr.', 'Rutledge', 'is', 'in;', 'He', 'means', 'to', 'destroy', 'our', 'Liberties', 'and', 'Rights.', '--', 'Charles', 'Scripter', '*', 'cescript@phy.mtu.edu', 'Dept', 'of', 'Physics,', 'Michigan', 'Tech,', 'Houghton,', 'MI', '49931', '-------------------------------------------------------------', '\"...when', 'all', 'government...', 'in', 'little', 'as', 'in', 'great', 'things,', 'shall', 'be', 'drawn', 'to', 'Washington', 'as', 'the', 'centre', 'of', 'all', 'power,', 'it', 'will', 'render', 'powerless', 'the', 'checks', 'provided', 'of', 'one', 'government', 'on', 'another', 'and', 'will', 'become', 'as', 'venal', 'and', 'oppressive', 'as', 'the', 'government', 'from', 'which', 'we', 'separated.\"', 'Thomas', 'Jefferson,', '1821'], tags=['Train_1'])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd13d2",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "We'll instantiate a Doc2Vec model-Distributed Bag of Words (DBOW). In the Word2Vec architecture, the two algorithm names are ‚Äúcontinuous bag of words‚Äù (cbow) and ‚Äúskip-gram‚Äù (sg); in the Doc2Vec architecture, the corresponding algorithms are ‚Äúdistributed bag of words‚Äù (dbow) and ‚Äúdistributed memory‚Äù (dm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ad398",
   "metadata": {},
   "source": [
    "### DBOW\n",
    "\n",
    "DBOW is the Doc2Vec model analogous to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "Training a Doc2Vec model is rather straight forward in Gensim, we initialize the model and train for 30 epochs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d457e7",
   "metadata": {},
   "source": [
    "dm =0 means ‚Äòdistributed bag of words‚Äô (DBOW), set min_count=2 means ignoring all words with total frequency lower than this, size=100 is dimensionality of the generated feature vectors, alpha=0.025 is the initial alpha rate, learning rate will linearly drop to min_alpha as training progresses. And then we build a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "059f2d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03c20cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1134138.20it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1130621.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1130621.07it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 223564.32it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1128736.25it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1131160.74it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1130081.91it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<00:00, 1127930.40it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1130/1130 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 17.8 s\n",
      "Wall time: 6.65 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31dad7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac25d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "228309c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= {\n",
    "#          'nb':MultinomialNB(),\n",
    "         'lrDoc2Vec': LogisticRegression(),\n",
    "#          'rf': RandomForestClassifier( random_state=49), \n",
    "         'dtcDoc2Vec': DecisionTreeClassifier(),\n",
    "#          'svc': SVC(),\n",
    "#          'sgdword2Vec': SGDClassifier(tol=1e-3),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41c31eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_parameter_grid = {\n",
    "                           'lrDoc2Vec':{\n",
    "                                          \"lrDoc2Vec__penalty\": [ 'none', 'l2', ], # 'none', 'elasticnet'\n",
    "                                          \"lrDoc2Vec__C\": [0.001, 0.01, 0.1, 0.5, 1, 1.5],\n",
    "                                          # \"lr__solver\": ['newton-cg', 'lbfgs', 'liblinear'], #'sag', 'saga'],\n",
    "                                          \"lrDoc2Vec__max_iter\": [500],\n",
    "                                          \"lrDoc2Vec__multi_class\": ['auto'],\n",
    "                                          \"lrDoc2Vec__n_jobs\": [-1],\n",
    "                                          # \"lr__l1_ratio\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "                        },\n",
    "                           'dtDoc2Vec': {\n",
    "                                           \"dtDoc2Vec__min_samples_split\":[2,4],\n",
    "                            \n",
    "                        },\n",
    "#                         'rf': {\n",
    "#                                # \"rf__criterion\": [ 'gini', 'entropy', 'log_loss'],\n",
    "#                                # \"rf__n_estimators\": [20, 50, 100],\n",
    "#                                \"rf__max_depth\": [None, 2, 4, 6],\n",
    "#                                # \"rf__min_samples_split\": [2,5, 10, 15, 20, 30, 50],\n",
    "#                               },\n",
    "#                         'nb': {  \n",
    "#                         },\n",
    "#                         'svc': {\n",
    "#                                 \"svc__C\": [ 0, 0.5],\n",
    "#                                 # \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#                                 # \"svc__gamma\": ['scale', 'auto'],\n",
    "#                                 #\"svc__shrinking\": [True, False],\n",
    "#                                }\n",
    "                          'dtcDoc2Vec': {\n",
    "                                           \"dtcDoc2Vec__min_samples_split\":[2,4],\n",
    "                            \n",
    "                        },\n",
    "#                         'sgdword2Vec': {\n",
    "#                                 'sgdword2Vec__max_iter': (20,),\n",
    "#                                 'sgdword2Vec__alpha': (0.00001, 0.000001),\n",
    "#                                 'sgd_word2Vec__penalty': ('l2', 'elasticnet'),\n",
    "#                         },\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca02bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Processing 2 models. Please, wait...\n",
      "\n",
      "Algorithm being processed: lrDoc2Vec\n",
      "Fitting 2 folds for each of 12 candidates, totalling 24 fits\n",
      "Best score: 0.820 \n",
      "\n",
      "\n",
      "Algorithm being processed: dtcDoc2Vec\n",
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
      "Best score: 0.708 \n",
      "\n",
      "[+] Finish Processing\n",
      "\n",
      "CPU times: total: 750 ms\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results_dict={}\n",
    "cv = 2            # Cross validation\n",
    "\n",
    "# comparison_matrix = pd.DataFrame(columns=['Model', \n",
    "#                                           'Pipeline', \n",
    "#                                           'Best_Estimator',\n",
    "#                                           'Best_Accuracy'])\n",
    "\n",
    "print(\"[+] Processing {} models. Please, wait...\".format(len(models)))\n",
    "\n",
    "for model in models:\n",
    "    print(\"\\nAlgorithm being processed: {}\".format(model))\n",
    "    pipeline = Pipeline([(model, models[model])])  \n",
    "    grid_search_pipe = GridSearchCV(pipeline, \n",
    "                                param_grid=doc2vec_parameter_grid[model], \n",
    "                                cv=cv,\n",
    "                                n_jobs=-1, \n",
    "                                verbose=1)\n",
    "    grid_search_pipe.fit(X_train_vect_avg, y_train)\n",
    "\n",
    "    print(\"Best score: %0.3f \\n\" % grid_search_pipe.best_score_)\n",
    "#     print(\"Best Parameters: {} \\n\".format( grid_search_pipe.best_estimator_.get_params()))\n",
    "    best_parameters = grid_search_pipe.best_estimator_.get_params()\n",
    "    #print(\"Best estimator: {} \\n\".format( grid_search_pipe.best_estimator_))\n",
    "    series_aux =pd.Series(data=[model,\n",
    "                                models[model],\n",
    "                                grid_search_pipe.best_estimator_,\n",
    "                                grid_search_pipe.best_score_],\n",
    "                          index=comparison_matrix.columns)\n",
    "    \n",
    "\n",
    "        \n",
    "    comparison_matrix = comparison_matrix.append(series_aux, \n",
    "                                                 ignore_index=True)\n",
    "\n",
    "print(\"[+] Finish Processing\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfcb978b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>Best_Estimator</th>\n",
       "      <th>Best_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lrWord2Vec</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>(LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.819690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dtcWord2Vec</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>(DecisionTreeClassifier(min_samples_split=4))</td>\n",
       "      <td>0.710177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lrDoc2Vec</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>(LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.819690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dtcDoc2Vec</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>(DecisionTreeClassifier())</td>\n",
       "      <td>0.707965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model                  Pipeline  \\\n",
       "0   lrWord2Vec      LogisticRegression()   \n",
       "1  dtcWord2Vec  DecisionTreeClassifier()   \n",
       "2    lrDoc2Vec      LogisticRegression()   \n",
       "3   dtcDoc2Vec  DecisionTreeClassifier()   \n",
       "\n",
       "                                                           Best_Estimator  \\\n",
       "0  (LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "1                           (DecisionTreeClassifier(min_samples_split=4))   \n",
       "2  (LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "3                                              (DecisionTreeClassifier())   \n",
       "\n",
       "   Best_Accuracy  \n",
       "0       0.819690  \n",
       "1       0.710177  \n",
       "2       0.819690  \n",
       "3       0.707965  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69835922",
   "metadata": {},
   "source": [
    "### CountVectorizer and TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8ee0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models= {\n",
    "         'nb':MultinomialNB(),\n",
    "         'lr': LogisticRegression(),\n",
    "#          'rf': RandomForestClassifier( random_state=49), \n",
    "         'dt': DecisionTreeClassifier(),\n",
    "#          'svc': SVC(),\n",
    "         'sgd': SGDClassifier(tol=1e-3),\n",
    "        }\n",
    "\n",
    "feature_extractor_pipelines = {\n",
    "                                \"Pipeline_CountVectorizer\":[('vect', CountVectorizer())],\n",
    "                                \"Pipeline_TFIDF\": [('vect', CountVectorizer()),\n",
    "                                                   ('tfidf', TfidfTransformer())],                                         \n",
    "                              }\n",
    "\n",
    "# \"master_parameter_grid\" dictionary stores the parameters fr all the models that support Pipeline\n",
    "master_parameter_grid = {\n",
    "                        'vect': {\n",
    "                                 'vect__max_df': (0.5, 0.75, 1.0),\n",
    "                                 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "                                 'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams   \n",
    "                                },\n",
    "                        'tfidf': {\n",
    "                                  'tfidf__use_idf': (True, False),\n",
    "                                  'tfidf__norm': ('l1', 'l2'),\n",
    "                                },\n",
    "                        'lr':{\n",
    "                              \"lr__penalty\": [ 'none', 'l2', ], # 'none', 'elasticnet'\n",
    "                              # \"lr__C\": [0.001, 0.01, 0.1, 0.5, 1, 1.5],\n",
    "                              # \"lr__solver\": ['newton-cg', 'lbfgs', 'liblinear'], #'sag', 'saga'],\n",
    "                              \"lr__max_iter\": [500],\n",
    "                              \"lr__multi_class\": ['auto'],\n",
    "                              \"lr__n_jobs\": [-1],\n",
    "                              # \"lr__l1_ratio\": [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "                        },\n",
    "#                         'rf': {\n",
    "#                                # \"rf__criterion\": [ 'gini', 'entropy', 'log_loss'],\n",
    "#                                # \"rf__n_estimators\": [20, 50, 100],\n",
    "#                                \"rf__max_depth\": [None, 2, 4, 6],\n",
    "#                                # \"rf__min_samples_split\": [2,5, 10, 15, 20, 30, 50],\n",
    "#                               },\n",
    "                        'nb': { \n",
    "                                \"nb__alpha\":[0,0,5,1]\n",
    "                        },\n",
    "#                         'svc': {\n",
    "#                                 \"svc__C\": [ 0, 0.5],\n",
    "#                                 # \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#                                 # \"svc__gamma\": ['scale', 'auto'],\n",
    "#                                 #\"svc__shrinking\": [True, False],\n",
    "#                                }\n",
    "#                         'dt': {\n",
    "#                                \"dt__min_samples_split\":[2,4],\n",
    "                            \n",
    "#                         },\n",
    "#                         'sgd': {\n",
    "#                                 'clf__max_iter': (20,),\n",
    "#                                 'clf__alpha': (0.00001, 0.000001),\n",
    "#                                 'clf__penalty': ('l2', 'elasticnet'),\n",
    "#                         },\n",
    "                        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6278212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Processing 4 models with 2 feature extractors strategies per model. Total = 8.\n",
      "Please, wait...\n",
      "\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('nb', MultinomialNB())])\n",
      "Fitting 2 folds for each of 96 candidates, totalling 192 fits\n",
      "Best score: 0.997 \n",
      "\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('nb', MultinomialNB())])\n",
      "Fitting 2 folds for each of 384 candidates, totalling 768 fits\n",
      "Best score: 0.994 \n",
      "\n",
      "Model nb finished processing 2 feature extractors\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('lr', LogisticRegression())])\n",
      "Fitting 2 folds for each of 48 candidates, totalling 96 fits\n",
      "Best score: 0.984 \n",
      "\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('lr', LogisticRegression())])\n",
      "Fitting 2 folds for each of 192 candidates, totalling 384 fits\n",
      "Best score: 0.990 \n",
      "\n",
      "Model lr finished processing 2 feature extractors\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('dt', DecisionTreeClassifier())])\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "Best score: 0.958 \n",
      "\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('dt', DecisionTreeClassifier())])\n",
      "Fitting 2 folds for each of 96 candidates, totalling 192 fits\n",
      "Best score: 0.953 \n",
      "\n",
      "Model dt finished processing 2 feature extractors\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('sgd', SGDClassifier())])\n",
      "Fitting 2 folds for each of 24 candidates, totalling 48 fits\n",
      "Best score: 0.981 \n",
      "\n",
      "Pipeline: Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
      "                ('sgd', SGDClassifier())])\n",
      "Fitting 2 folds for each of 96 candidates, totalling 192 fits\n",
      "Best score: 0.990 \n",
      "\n",
      "Model sgd finished processing 2 feature extractors\n",
      "CPU times: total: 23 s\n",
      "Wall time: 3min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results_dict= {}  # Dictionary of results\n",
    "# cv = 2            # Cross validation\n",
    "\n",
    "print(\"[+] Processing {} models with {} feature extractors strategies per model. Total = {}.\\nPlease, wait...\\n\".\n",
    "                        format(len(models), len(feature_extractor_pipelines), len(models)*len(feature_extractor_pipelines)))\n",
    "\n",
    "for model in models:\n",
    "    dict_aux={}\n",
    "    \n",
    "    for feature_extractor_pipe in feature_extractor_pipelines.values():\n",
    "        steps = feature_extractor_pipe + [(model, models[model])]       # adds the classifier model to the current pipeline \n",
    "        pipeline = Pipeline(steps)                                      # of extractors\n",
    "        print(\"Pipeline: {}\".format(pipeline))\n",
    "        \n",
    "        # parameters ={} --> Dictionay of parameters that will be built in the next for loop. It contains the \n",
    "        # extractors parameters + classifier parameters defined in the \"master_parameter_grid\" dictionary \n",
    "        parameters ={}       \n",
    "                             \n",
    "        for step in steps:    \n",
    "            # \"step[0]\" is the name to identify the pipeline steps and is stored as the key in \"master_parameter_grid \n",
    "            if step[0] in master_parameter_grid:  \n",
    "                # the final parameter grid will be built according to the steps in the current pipeline\n",
    "                parameters = {**parameters, **master_parameter_grid[step[0]] }  # ** operator merges two dictionaries \n",
    "        grid_search_pipe = GridSearchCV(pipeline, \n",
    "                                        param_grid=parameters, \n",
    "                                        cv=cv,\n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=1)\n",
    "        grid_search_pipe.fit(data.data, data.target)\n",
    "        print(\"Best score: %0.3f \\n\" % grid_search_pipe.best_score_)\n",
    "        # print(\"Best Parameters: {} \\n\".format( grid_search.best_estimator_.get_params()))\n",
    "        # print(\"Best estimator: {} \\n\".format( grid_search_pipe.best_estimator_))\n",
    "\n",
    "        dict_aux[pipeline] = [grid_search_pipe.best_score_, grid_search_pipe.best_estimator_]\n",
    "    \n",
    "    results_dict[model] = dict_aux\n",
    "    print(\"Model {} finished processing {} feature extractors\".format(model, len(feature_extractor_pipelines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07849cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Finish Processing\n"
     ]
    }
   ],
   "source": [
    "for dict in results_dict:\n",
    "    for pipe in results_dict[dict]:\n",
    "        series_aux =pd.Series(data=[                        # ['Model', 'Pipeline', 'Best_Estimator','Best_Accuracy']\n",
    "                                    dict, \n",
    "                                    pipe, \n",
    "                                    results_dict[dict][pipe][1], \n",
    "                                    results_dict[dict][pipe][0]\n",
    "                                   ],  \n",
    "                              index=comparison_matrix.columns)       \n",
    "        comparison_matrix = comparison_matrix.append(series_aux, \n",
    "                                                     ignore_index=True)\n",
    "print(\"[+] Finish Processing\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1440adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Pipeline</th>\n",
       "      <th>Best_Estimator</th>\n",
       "      <th>Best_Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb</td>\n",
       "      <td>(CountVectorizer(), MultinomialNB())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, max_features=10000), MultinomialNB(alpha=1))</td>\n",
       "      <td>0.997345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb</td>\n",
       "      <td>(CountVectorizer(), TfidfTransformer(), MultinomialNB())</td>\n",
       "      <td>(CountVectorizer(max_df=0.75, max_features=10000, ngram_range=(1, 2)), TfidfTransformer(norm='l1', use_idf=False), MultinomialNB(alpha=0))</td>\n",
       "      <td>0.993805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lr</td>\n",
       "      <td>(CountVectorizer(), TfidfTransformer(), LogisticRegression())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, max_features=10000), TfidfTransformer(), LogisticRegression(max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.990265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sgd</td>\n",
       "      <td>(CountVectorizer(), TfidfTransformer(), SGDClassifier())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5), TfidfTransformer(), SGDClassifier())</td>\n",
       "      <td>0.990265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lr</td>\n",
       "      <td>(CountVectorizer(), LogisticRegression())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, max_features=10000), LogisticRegression(max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.984071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sgd</td>\n",
       "      <td>(CountVectorizer(), SGDClassifier())</td>\n",
       "      <td>(CountVectorizer(max_df=0.75, ngram_range=(1, 2)), SGDClassifier())</td>\n",
       "      <td>0.980531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dt</td>\n",
       "      <td>(CountVectorizer(), DecisionTreeClassifier())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, max_features=10000, ngram_range=(1, 2)), DecisionTreeClassifier())</td>\n",
       "      <td>0.957522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dt</td>\n",
       "      <td>(CountVectorizer(), TfidfTransformer(), DecisionTreeClassifier())</td>\n",
       "      <td>(CountVectorizer(max_df=0.5, max_features=10000), TfidfTransformer(norm='l1'), DecisionTreeClassifier())</td>\n",
       "      <td>0.953097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lrWord2Vec</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>(LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.819690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lrDoc2Vec</td>\n",
       "      <td>LogisticRegression()</td>\n",
       "      <td>(LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))</td>\n",
       "      <td>0.819690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dtcWord2Vec</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>(DecisionTreeClassifier(min_samples_split=4))</td>\n",
       "      <td>0.710177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dtcDoc2Vec</td>\n",
       "      <td>DecisionTreeClassifier()</td>\n",
       "      <td>(DecisionTreeClassifier())</td>\n",
       "      <td>0.707965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  \\\n",
       "0            nb   \n",
       "1            nb   \n",
       "2            lr   \n",
       "3           sgd   \n",
       "4            lr   \n",
       "5           sgd   \n",
       "6            dt   \n",
       "7            dt   \n",
       "8    lrWord2Vec   \n",
       "9     lrDoc2Vec   \n",
       "10  dtcWord2Vec   \n",
       "11   dtcDoc2Vec   \n",
       "\n",
       "                                                             Pipeline  \\\n",
       "0                                (CountVectorizer(), MultinomialNB())   \n",
       "1            (CountVectorizer(), TfidfTransformer(), MultinomialNB())   \n",
       "2       (CountVectorizer(), TfidfTransformer(), LogisticRegression())   \n",
       "3            (CountVectorizer(), TfidfTransformer(), SGDClassifier())   \n",
       "4                           (CountVectorizer(), LogisticRegression())   \n",
       "5                                (CountVectorizer(), SGDClassifier())   \n",
       "6                       (CountVectorizer(), DecisionTreeClassifier())   \n",
       "7   (CountVectorizer(), TfidfTransformer(), DecisionTreeClassifier())   \n",
       "8                                                LogisticRegression()   \n",
       "9                                                LogisticRegression()   \n",
       "10                                           DecisionTreeClassifier()   \n",
       "11                                           DecisionTreeClassifier()   \n",
       "\n",
       "                                                                                                                                Best_Estimator  \\\n",
       "0                                                                    (CountVectorizer(max_df=0.5, max_features=10000), MultinomialNB(alpha=1))   \n",
       "1   (CountVectorizer(max_df=0.75, max_features=10000, ngram_range=(1, 2)), TfidfTransformer(norm='l1', use_idf=False), MultinomialNB(alpha=0))   \n",
       "2           (CountVectorizer(max_df=0.5, max_features=10000), TfidfTransformer(), LogisticRegression(max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "3                                                                           (CountVectorizer(max_df=0.5), TfidfTransformer(), SGDClassifier())   \n",
       "4                               (CountVectorizer(max_df=0.5, max_features=10000), LogisticRegression(max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "5                                                                          (CountVectorizer(max_df=0.75, ngram_range=(1, 2)), SGDClassifier())   \n",
       "6                                              (CountVectorizer(max_df=0.5, max_features=10000, ngram_range=(1, 2)), DecisionTreeClassifier())   \n",
       "7                                     (CountVectorizer(max_df=0.5, max_features=10000), TfidfTransformer(norm='l1'), DecisionTreeClassifier())   \n",
       "8                                                                       (LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "9                                                                       (LogisticRegression(C=0.001, max_iter=500, n_jobs=-1, penalty='none'))   \n",
       "10                                                                                               (DecisionTreeClassifier(min_samples_split=4))   \n",
       "11                                                                                                                  (DecisionTreeClassifier())   \n",
       "\n",
       "    Best_Accuracy  \n",
       "0        0.997345  \n",
       "1        0.993805  \n",
       "2        0.990265  \n",
       "3        0.990265  \n",
       "4        0.984071  \n",
       "5        0.980531  \n",
       "6        0.957522  \n",
       "7        0.953097  \n",
       "8        0.819690  \n",
       "9        0.819690  \n",
       "10       0.710177  \n",
       "11       0.707965  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_matrix.sort_values(['Best_Accuracy'], \n",
    "                              ascending=False, \n",
    "                              ignore_index=True,\n",
    "                              inplace=True)\n",
    "comparison_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9f491a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_matrix.to_csv('Jose_Lira_Task01_Text_Classification.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355dc698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
